{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d5d6c24",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ac7b5",
   "metadata": {},
   "source": [
    "## [Let's Play first!](https://playground.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0a091",
   "metadata": {},
   "source": [
    "![TF](figures/gdn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf6174",
   "metadata": {},
   "source": [
    "## For nonlinear problems\n",
    "\n",
    "![Neuron](figures/lgno.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85931413",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba1909f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.11\n",
      "NumPy version: 1.21.5\n",
      "pytorch version: 1.11.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "assert platform.python_version_tuple() >= (\"3\", \"6\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Setup plots\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = 10, 8\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "print(f\"pytorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6193538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def plot_planar_data(X, y):\n",
    "    \"\"\"Plot some 2D data\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(X[y == 0, 0], X[y == 0, 1], \"or\", alpha=0.5, label=0)\n",
    "    plt.plot(X[y == 1, 0], X[y == 1, 1], \"ob\", alpha=0.5, label=1)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_decision_boundary(pred_func, X, y, figure=None):\n",
    "    \"\"\"Plot a decision boundary\"\"\"\n",
    "\n",
    "    if figure is None:  # If no figure is given, create a new one\n",
    "        plt.figure()\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    \"\"\"Plot training and (optionally) validation loss and accuracy\n",
    "    Takes a Keras History object as parameter\"\"\"\n",
    "\n",
    "    loss = history.history[\"loss\"]\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs, loss, \".--\", label=\"Training loss\")\n",
    "    final_loss = loss[-1]\n",
    "    title = \"Training loss: {:.4f}\".format(final_loss)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    if \"val_loss\" in history.history:\n",
    "        val_loss = history.history[\"val_loss\"]\n",
    "        plt.plot(epochs, val_loss, \"o-\", label=\"Validation loss\")\n",
    "        final_val_loss = val_loss[-1]\n",
    "        title += \", Validation loss: {:.4f}\".format(final_val_loss)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    acc = history.history[\"accuracy\"]\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs, acc, \".--\", label=\"Training acc\")\n",
    "    final_acc = acc[-1]\n",
    "    title = \"Training accuracy: {:.2f}%\".format(final_acc * 100)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    if \"val_accuracy\" in history.history:\n",
    "        val_acc = history.history[\"val_accuracy\"]\n",
    "        plt.plot(epochs, val_acc, \"o-\", label=\"Validation acc\")\n",
    "        final_val_acc = val_acc[-1]\n",
    "        title += \", Validation accuracy: {:.2f}%\".format(final_val_acc * 100)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c65b0",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd268ef",
   "metadata": {},
   "source": [
    "### A biological inspiration\n",
    "\n",
    "![Neuron](figures/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c829a",
   "metadata": {},
   "source": [
    "### McCulloch & Pitts' formal neuron (1943)\n",
    "\n",
    "![Formal neuron model](figures/neuron_model.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b64fd4",
   "metadata": {},
   "source": [
    "### Hebb's rule (1949)\n",
    "\n",
    "Attempt to explain synaptic plasticity, the adaptation of brain neurons during the learning process.\n",
    "\n",
    "> \"The general idea is an old one, that any two cells or systems of cells that are repeatedly active at the same time will tend to become 'associated' so that activity in one facilitates activity in the other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde34d7f",
   "metadata": {},
   "source": [
    "### Franck Rosenblatt's perceptron (1958)\n",
    "\n",
    "![The Perceptron](figures/Perceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3935ce7",
   "metadata": {},
   "source": [
    "### The perceptron learning algorithm\n",
    "\n",
    "1. Init randomly the $\\theta$ connection weights.\n",
    "1. For each training sample $x^{(i)}$:\n",
    "    1. Compute the perceptron output $y'^{(i)}$\n",
    "    1. Adjust weights : $\\theta_{next} = \\theta + \\eta (y^{(i)} - y'^{(i)}) x^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dbe22f",
   "metadata": {},
   "source": [
    "### The MultiLayer Perceptron (MLP)\n",
    "\n",
    "![MultiLayer Perceptron](figures/neural_net2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc340c",
   "metadata": {},
   "source": [
    "![MultiLayer Perceptron](figures/nng.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fb2b7",
   "metadata": {},
   "source": [
    "### Minsky's critic (1969)\n",
    "\n",
    "One perceptron cannot learn non-linearly separable functions.\n",
    "\n",
    "![XOR problem](figures/xor.png)\n",
    "\n",
    "At the time, no learning algorithm existed for training the hidden layers of a MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61bb12",
   "metadata": {},
   "source": [
    "![MultiLayer Perceptron](figures/msg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1c94e",
   "metadata": {},
   "source": [
    "![MultiLayer Perceptron](figures/msg2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0c9a6",
   "metadata": {},
   "source": [
    "### Decisive breakthroughs (1970s-1990s)\n",
    "\n",
    "- 1974: backpropagation theory (P. Werbos).\n",
    "- 1986: learning through backpropagation (Rumelhart, Hinton, Williams).\n",
    "- 1991: universal approximation theorem (Hornik, Stinchcombe, White).\n",
    "- 1989: first researchs on deep neural nets (LeCun, Bengio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9140dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afb33bea",
   "metadata": {},
   "source": [
    "### Anatomy of a network\n",
    "\n",
    "![A neural network](figures/nn_weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcaa422",
   "metadata": {},
   "source": [
    "### Neuron output\n",
    "\n",
    "![Neuron output](figures/neuron_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a87516",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "- Applied to the weighted sum of neuron inputs to produce its output.\n",
    "- Always non-linear. If not, the whole network could only apply a linear transformation to its inputs and couldn't solve complex problems.\n",
    "- The main ones are:\n",
    "  - **sigmoid** (*logistic function*)\n",
    "  - **tanh** (*hyberbolic tangent*)\n",
    "  - **ReLU** (*Rectified Linear Unit*)\n",
    "- See [Activation functions](../reference/activation_functions) for details.\n",
    "![MultiLayer Perceptron](figures/act.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03288d8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b26206b5",
   "metadata": {},
   "source": [
    "### Single neuron classifier\n",
    "\n",
    "Equivalent to [logistic regression](logistic_regression).\n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol{\\pmb{\\theta}}) = -\\frac{1}{m}\\sum_{i=1}^m \\left(y^{(i)} \\log_e(y'^{(i)}) + (1-y^{(i)}) \\log_e(1-y'^{(i)})\\right)$$\n",
    "\n",
    "[![Neural classifier](figures/neural_classifier.png)](https://youtu.be/FBggC-XVF4M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad01bf3",
   "metadata": {},
   "source": [
    "### Single layer multiclass classifier\n",
    "\n",
    "Equivalent to softmax regression.\n",
    "\n",
    "$$\\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^K {e^{z_k}}}\\;\\;\\;\\;\n",
    "\\mathcal{L}(\\boldsymbol{\\pmb{\\theta}}) = -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K y^{(i)}_k \\log_e(y'^{(i)}_k)$$\n",
    "\n",
    "[![Multiclass neural classifier](figures/multiclass_neural_classifier.png)](https://youtu.be/FBggC-XVF4M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daefac3",
   "metadata": {},
   "source": [
    "### Universal approximation theorem (1991)\n",
    "\n",
    "- The hidden layers of a neural network transform their input space.\n",
    "- A network can be seen as a series of non-linear compositions applied to the input data.\n",
    "- Given appropriate complexity and appropriate learning, a network can theorically approximate any continuous function.\n",
    "- One of the most important theoretical results for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a10c68",
   "metadata": {},
   "source": [
    "### Training and inference\n",
    "\n",
    "![Training and inference](figures/training_inference1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d962b24",
   "metadata": {},
   "source": [
    "![MultiLayer Perceptron](figures/hype.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a5a8b",
   "metadata": {},
   "source": [
    "### Learning algorithm\n",
    "\n",
    "[![Extract from the book Deep Learning with Python](figures/nn_learning.jpg)](https://www.manning.com/books/deep-learning-with-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679b587",
   "metadata": {},
   "source": [
    "![MultiLayer Perceptron](figures/gd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba4eaf",
   "metadata": {},
   "source": [
    "### Weights initialization\n",
    "\n",
    "To facilitate training, initial weights must be:\n",
    "\n",
    "- non-zero\n",
    "- random\n",
    "- have small values\n",
    "\n",
    "[Several techniques exist](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79). A commonly used one is [Xavier initialization](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19553581",
   "metadata": {},
   "source": [
    "### Vectorization of computations\n",
    "\n",
    "![NN matrixes](figures/nn_matrixes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1718a57",
   "metadata": {},
   "source": [
    "![NN matrixes](figures/nng.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e31817",
   "metadata": {},
   "source": [
    "#### Hidden layer 1 output\n",
    "\n",
    "![Layer 1 output](figures/output_layer1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb5083",
   "metadata": {},
   "source": [
    "#### Hidden layer 2 output\n",
    "\n",
    "![Layer 2 output](figures/output_layer2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c96da",
   "metadata": {},
   "source": [
    "#### Output layer result\n",
    "\n",
    "![Layer 3 output](figures/output_layer3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e3eae",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Objective: compute $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta})$, the loss function gradient w.r.t. all the network weights.\n",
    "\n",
    "Method: apply the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) to compute partial derivatives backwards, starting from the current output.\n",
    "\n",
    "$$y = f(g(x)) \\;\\;\\;\\; \\frac{\\partial y}{\\partial x} = \\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial x}\\;\\;\\;\\; \\frac{\\partial y}{\\partial x} = \\sum_{i=1}^n \\frac{\\partial f}{\\partial g^{(i)}} \\frac{\\partial g^{(i)}}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f56d0",
   "metadata": {},
   "source": [
    "![NN matrixes](figures/pg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43b5f9",
   "metadata": {},
   "source": [
    "![NN matrixes](figures/backt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41160b",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "- It’s a Python based scientific computing package targeted at two sets of audiences:\n",
    "    - A replacement for NumPy to use the power of GPUs\n",
    "    - Deep learning research platform that provides maximum flexibility and speed\n",
    "- pros: \n",
    "    - Interactively debugging PyTorch. Many users who have used both frameworks would argue that makes pytorch significantly easier to debug and visualize.\n",
    "    - Clean support for dynamic graphs\n",
    "    - Organizational backing from Facebook\n",
    "    - Blend of high level and low level APIs\n",
    "- cons:\n",
    "    - Much less mature than alternatives\n",
    "    - Limited references / resources outside of the official documentation\n",
    "- I accept you know neural network basics. If you do not know check my tutorial. Because I will not explain neural network concepts detailed, I only explain how to use pytorch for neural network\n",
    "- Neural Network tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners \n",
    "- The most important parts of this tutorial from matrices to ANN. If you learn these parts very well, implementing remaining parts like CNN or RNN will be very easy. \n",
    "<br>\n",
    "<br>**Content:**\n",
    "1. [Basics of Pytorch](#1)\n",
    "    - Matrices\n",
    "    - Math\n",
    "    - Variable\n",
    "1. [Linear Regression](#2)\n",
    "1. [Logistic Regression](#3)\n",
    "1. [Artificial Neural Network (ANN)](#4)\n",
    "1. [Concolutional Neural Network (CNN)](#5)\n",
    "1. Recurrent Neural Network (RNN)\n",
    "    - https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch\n",
    "1. Long-Short Term Memory (LSTM)\n",
    "    - https://www.kaggle.com/kanncaa1/long-short-term-memory-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e8b5ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iris', 'svm.jpeg', 'titanic_train.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./data\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3ed63",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## Basics of Pytorch\n",
    "### Matrices\n",
    "- In pytorch, matrix(array) is called tensors.\n",
    "- 3*3 matrix koy. This is 3x3 tensor.\n",
    "- Lets look at array example with numpy that we already know.\n",
    "    - We create numpy array with np.numpy() method\n",
    "    - Type(): type of the array. In this example it is numpy\n",
    "    - np.shape(): shape of the array. Row x Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53ef62ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Type: <class 'numpy.ndarray'>\n",
      "Array Shape: (2, 3)\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy library\n",
    "import numpy as np\n",
    "\n",
    "# numpy array\n",
    "array = [[1,2,3],[4,5,6]]\n",
    "first_array = np.array(array) # 2x3 array\n",
    "print(\"Array Type: {}\".format(type(first_array))) # type\n",
    "print(\"Array Shape: {}\".format(np.shape(first_array))) # shape\n",
    "print(first_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b350e",
   "metadata": {},
   "source": [
    "- We looked at numpy array.\n",
    "- Now examine how we implement tensor(pytorch array)\n",
    "- import pytorch library with import torch\n",
    "- We create tensor with torch.Tensor() method\n",
    "- type: type of the array. In this example it is tensor\n",
    "- shape: shape of the array. Row x Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed0cd251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Type: <built-in method type of Tensor object at 0x000001E105754408>\n",
      "Array Shape: torch.Size([2, 3])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# import pytorch library\n",
    "import torch\n",
    "\n",
    "# pytorch array\n",
    "tensor = torch.Tensor(array)\n",
    "print(\"Array Type: {}\".format(tensor.type)) # type\n",
    "print(\"Array Shape: {}\".format(tensor.shape)) # shape\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1cb7bb",
   "metadata": {},
   "source": [
    "- Allocation is one of the most used technique in coding. Therefore lets learn how to make it with pytorch.\n",
    "- In order to learn, compare numpy and tensor\n",
    "    - np.ones() = torch.ones()\n",
    "    - np.random.rand() = torch.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81fa9c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy [[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Numpy [[0.71906628 0.75122874 0.46073038]\n",
      " [0.07985004 0.86760214 0.88665102]]\n",
      "\n",
      "tensor([[0.7781, 0.3178, 0.7480],\n",
      "        [0.4609, 0.2787, 0.0737]])\n"
     ]
    }
   ],
   "source": [
    "# numpy ones\n",
    "print(\"Numpy {}\\n\".format(np.ones((2,3))))\n",
    "\n",
    "# pytorch ones\n",
    "print(torch.ones((2,3)))\n",
    "\n",
    "# numpy random\n",
    "print(\"Numpy {}\\n\".format(np.random.rand(2,3)))\n",
    "\n",
    "# pytorch random\n",
    "print(torch.rand(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0dc0b",
   "metadata": {},
   "source": [
    "- Even if when I use pytorch for neural networks, I feel better if I use numpy. Therefore, usually convert result of neural network that is tensor to numpy array to visualize or examine.\n",
    "- Lets look at conversion between tensor and numpy arrays.\n",
    "    - torch.from_numpy(): from numpy to tensor\n",
    "    - numpy(): from tensor to numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9a8d3e",
   "metadata": {},
   "source": [
    "# random numpy array\n",
    "array = np.random.rand(2,2)\n",
    "print(\"{} {}\\n\".format(type(array),array))\n",
    "\n",
    "# from numpy to tensor\n",
    "from_numpy_to_tensor = torch.from_numpy(array)\n",
    "print(\"{}\\n\".format(from_numpy_to_tensor))\n",
    "\n",
    "# from tensor to numpy\n",
    "tensor = from_numpy_to_tensor\n",
    "from_tensor_to_numpy = tensor.numpy()\n",
    "print(\"{} {}\\n\".format(type(from_tensor_to_numpy),from_tensor_to_numpy))\n",
    "\n",
    "### Basic Math with Pytorch\n",
    "- Resize: view()\n",
    "- a and b are tensor.\n",
    "- Addition: torch.add(a,b) = a + b\n",
    "- Subtraction: a.sub(b) = a - b\n",
    "- Element wise multiplication: torch.mul(a,b) = a * b \n",
    "- Element wise division: torch.div(a,b) = a / b \n",
    "- Mean: a.mean()\n",
    "- Standart Deviation (std): a.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor \n",
    "tensor = torch.ones(3,3)\n",
    "print(\"\\n\",tensor)\n",
    "\n",
    "# Resize\n",
    "print(\"{}{}\\n\".format(tensor.view(9).shape,tensor.view(9)))\n",
    "\n",
    "# Addition\n",
    "print(\"Addition: {}\\n\".format(torch.add(tensor,tensor)))\n",
    "\n",
    "# Subtraction\n",
    "print(\"Subtraction: {}\\n\".format(tensor.sub(tensor)))\n",
    "\n",
    "# Element wise multiplication\n",
    "print(\"Element wise multiplication: {}\\n\".format(torch.mul(tensor,tensor)))\n",
    "\n",
    "# Element wise division\n",
    "print(\"Element wise division: {}\\n\".format(torch.div(tensor,tensor)))\n",
    "\n",
    "# Mean\n",
    "tensor = torch.Tensor([1,2,3,4,5])\n",
    "print(\"Mean: {}\".format(tensor.mean()))\n",
    "\n",
    "# Standart deviation (std)\n",
    "print(\"std: {}\".format(tensor.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c794f1b",
   "metadata": {},
   "source": [
    "### Variables\n",
    "- It accumulates gradients. \n",
    "- We will use pytorch in neural network. And as you know, in neural network we have backpropagation where gradients are calculated. Therefore we need to handle gradients. If you do not know neural network, check my deep learning tutorial first because I will not explain detailed the concepts like optimization, loss function or backpropagation. \n",
    "- Deep learning tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n",
    "- Difference between variables and tensor is variable accumulates gradients.\n",
    "- We can make math operations with variables, too.\n",
    "- In order to make backward propagation we need variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f1c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import variable from pytorch library\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# define variable\n",
    "var = Variable(torch.ones(3), requires_grad = True)\n",
    "var\n",
    "\n",
    "- Assume we have equation y = x^2\n",
    "- Define x = [2,4] variable\n",
    "- After calculation we find that y = [4,16] (y = x^2)\n",
    "- Recap o equation is that o = (1/2)*sum(y) = (1/2)*sum(x^2)\n",
    "- deriavative of o = x\n",
    "- Result is equal to x so gradients are [2,4]\n",
    "- Lets implement\n",
    "\n",
    "# lets make basic backward propagation\n",
    "# we have an equation that is y = x^2\n",
    "array = [2,4]\n",
    "tensor = torch.Tensor(array)\n",
    "x = Variable(tensor, requires_grad = True)\n",
    "y = x**2\n",
    "print(\" y =  \",y)\n",
    "\n",
    "# recap o equation o = 1/2*sum(y)\n",
    "o = (1/2)*sum(y)\n",
    "print(\" o =  \",o)\n",
    "\n",
    "# backward\n",
    "o.backward() # calculates gradients\n",
    "\n",
    "# As I defined, variables accumulates gradients. In this part there is only one variable x.\n",
    "# Therefore variable x should be have gradients\n",
    "# Lets look at gradients with x.grad\n",
    "print(\"gradients: \",x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f406b",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "### Linear Regression\n",
    "- Detailed linear regression tutorial is in my machine learning tutorial in part \"Regression\". I will not explain it in here detailed.\n",
    "- Linear Regression tutorial: https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners\n",
    "- y = Ax + B.\n",
    "    - A = slope of curve\n",
    "    - B = bias (point that intersect y-axis)\n",
    "- For example, we have car company. If the car price is low, we sell more car. If the car price is high, we sell less car. This is the fact that we know and we have data set about this fact.\n",
    "- The question is that what will be number of car sell if the car price is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a car company we collect this data from previous selling\n",
    "# lets define car prices\n",
    "car_prices_array = [3,4,5,6,7,8,9]\n",
    "car_price_np = np.array(car_prices_array,dtype=np.float32)\n",
    "car_price_np = car_price_np.reshape(-1,1)\n",
    "car_price_tensor = Variable(torch.from_numpy(car_price_np))\n",
    "\n",
    "# lets define number of car sell\n",
    "number_of_car_sell_array = [ 7.5, 7, 6.5, 6.0, 5.5, 5.0, 4.5]\n",
    "number_of_car_sell_np = np.array(number_of_car_sell_array,dtype=np.float32)\n",
    "number_of_car_sell_np = number_of_car_sell_np.reshape(-1,1)\n",
    "number_of_car_sell_tensor = Variable(torch.from_numpy(number_of_car_sell_np))\n",
    "\n",
    "# lets visualize our data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(car_prices_array,number_of_car_sell_array)\n",
    "plt.xlabel(\"Car Price $\")\n",
    "plt.ylabel(\"Number of Car Sell\")\n",
    "plt.title(\"Car Price$ VS Number of Car Sell\")\n",
    "plt.show()\n",
    "\n",
    "- Now this plot is our collected data\n",
    "- We have a question that is what will be number of car sell if the car price is 100$\n",
    "- In order to solve this question we need to use linear regression.\n",
    "- We need to line fit into this data. Aim is fitting line with minimum error.\n",
    "- **Steps of Linear Regression**\n",
    "    1. create LinearRegression class\n",
    "    1. define model from this LinearRegression class\n",
    "    1. MSE: Mean squared error\n",
    "    1. Optimization (SGD:stochastic gradient descent)\n",
    "    1. Backpropagation\n",
    "    1. Prediction\n",
    "- Lets implement it with Pytorch\n",
    "\n",
    "# Linear Regression with Pytorch\n",
    "\n",
    "# libraries\n",
    "import torch      \n",
    "from torch.autograd import Variable     \n",
    "import torch.nn as nn \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create class\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        # super function. It inherits from nn.Module and we can access everythink in nn.Module\n",
    "        super(LinearRegression,self).__init__()\n",
    "        # Linear function.\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# define model\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = LinearRegression(input_dim,output_dim) # input and output size are 1\n",
    "\n",
    "# MSE\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optimization (find parameters that minimize error)\n",
    "learning_rate = 0.02   # how fast we reach best parameters\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "# train model\n",
    "loss_list = []\n",
    "iteration_number = 1001\n",
    "for iteration in range(iteration_number):\n",
    "        \n",
    "    # optimization\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    # Forward to get output\n",
    "    results = model(car_price_tensor)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = mse(results, number_of_car_sell_tensor)\n",
    "    \n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # store loss\n",
    "    loss_list.append(loss.data)\n",
    "    \n",
    "    # print loss\n",
    "    if(iteration % 50 == 0):\n",
    "        print('epoch {}, loss {}'.format(iteration, loss.data))\n",
    "\n",
    "plt.plot(range(iteration_number),loss_list)\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "- Number of iteration is 1001.\n",
    "- Loss is almost zero that you can see from plot or loss in epoch number 1000.\n",
    "- Now we have a trained model.\n",
    "- While usign trained model, lets predict car prices.\n",
    "\n",
    "# predict our car price \n",
    "predicted = model(car_price_tensor).data.numpy()\n",
    "plt.scatter(car_prices_array,number_of_car_sell_array,label = \"original data\",color =\"red\")\n",
    "plt.scatter(car_prices_array,predicted,label = \"predicted data\",color =\"blue\")\n",
    "\n",
    "# predict if car price is 10$, what will be the number of car sell\n",
    "#predicted_10 = model(torch.from_numpy(np.array([10]))).data.numpy()\n",
    "#plt.scatter(10,predicted_10.data,label = \"car price 10$\",color =\"green\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Car Price $\")\n",
    "plt.ylabel(\"Number of Car Sell\")\n",
    "plt.title(\"Original vs Predicted values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b997db2",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "### Logistic Regression\n",
    "- Linear regression is not good at classification.\n",
    "- We use logistic regression for classification.\n",
    "- linear regression + logistic function(softmax) = logistic regression\n",
    "- Check my deep learning tutorial. There is detailed explanation of logistic regression. \n",
    "    - https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n",
    "- **Steps of Logistic Regression**\n",
    "    1. Import Libraries\n",
    "    1. Prepare Dataset\n",
    "        - We use MNIST dataset.\n",
    "        - There are 28*28 images and 10 labels from 0 to 9\n",
    "        - Data is not normalized so we divide each image to 255 that is basic normalization for images.\n",
    "        - In order to split data, we use train_test_split method from sklearn library\n",
    "        - Size of train data is 80% and size of test data is 20%.\n",
    "        - Create feature and target tensors. At the next parts we create variable from these tensors. As you remember we need to define variable for accumulation of gradients.\n",
    "        - batch_size = batch size means is that for example we have data and it includes 1000 sample. We can train 1000 sample in a same time or we can divide it 10 groups which include 100 sample and train 10 groups in order. Batch size is the group size. For example, I choose batch_size = 100, that means in order to train all data only once we have 336 groups. We train each groups(336) that have batch_size(quota) 100. Finally we train 33600 sample one time.\n",
    "        - epoch: 1 epoch means training all samples one time.\n",
    "        - In our example: we have 33600 sample to train and we decide our batch_size is 100. Also we decide epoch is 29(accuracy achieves almost highest value when epoch is 29). Data is trained 29 times. Question is that how many iteration do I need? Lets calculate: \n",
    "            - training data 1 times = training 33600 sample (because data includes 33600 sample) \n",
    "            - But we split our data 336 groups(group_size = batch_size = 100) our data \n",
    "            - Therefore, 1 epoch(training data only once) takes 336 iteration\n",
    "            - We have 29 epoch, so total iterarion is 9744(that is almost 10000 which I used)\n",
    "        - TensorDataset(): Data set wrapping tensors. Each sample is retrieved by indexing tensors along the first dimension.\n",
    "        - DataLoader(): It combines dataset and sample. It also provides multi process iterators over the dataset.\n",
    "        - Visualize one of the images in dataset\n",
    "    1. Create Logistic Regression Model\n",
    "        - Same with linear regression.\n",
    "        - However as you expect, there should be logistic function in model right?\n",
    "        - In pytorch, logistic function is in the loss function where we will use at next parts.\n",
    "    1. Instantiate Model\n",
    "        - input_dim = 28*28 # size of image px*px\n",
    "        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n",
    "        - create model\n",
    "    1. Instantiate Loss \n",
    "        - Cross entropy loss\n",
    "        - It calculates loss that is not surprise :)\n",
    "        - It also has softmax(logistic function) in it.\n",
    "    1. Instantiate Optimizer \n",
    "        - SGD Optimizer\n",
    "    1. Traning the Model\n",
    "    1. Prediction\n",
    "- As a result, as you can see from plot, while loss decreasing, accuracy(almost 85%) is increasing and our model is learning(training).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare Dataset\n",
    "# load data\n",
    "train = pd.read_csv(r\"../input/train.csv\",dtype = np.float32)\n",
    "\n",
    "# split data into features(pixels) and labels(numbers from 0 to 9)\n",
    "targets_numpy = train.label.values\n",
    "features_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n",
    "\n",
    "# train test split. Size of train data is 80% and size of test data is 20%. \n",
    "features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n",
    "                                                                             targets_numpy,\n",
    "                                                                             test_size = 0.2,\n",
    "                                                                             random_state = 42) \n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "featuresTest = torch.from_numpy(features_test)\n",
    "targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# batch_size, epoch and iteration\n",
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "num_epochs = n_iters / (len(features_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# visualize one of the images in data set\n",
    "plt.imshow(features_numpy[10].reshape(28,28))\n",
    "plt.axis(\"off\")\n",
    "plt.title(str(targets_numpy[10]))\n",
    "plt.savefig('graph.png')\n",
    "plt.show()\n",
    "\n",
    "# Create Logistic Regression Model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # Linear part\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # There should be logistic function right?\n",
    "        # However logistic function in pytorch is in loss function\n",
    "        # So actually we do not forget to put it, it is only at next parts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# Instantiate Model Class\n",
    "input_dim = 28*28 # size of image px*px\n",
    "output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n",
    "\n",
    "# create logistic regression model\n",
    "model = LogisticRegressionModel(input_dim, output_dim)\n",
    "\n",
    "# Cross Entropy Loss  \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer \n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Traning the Model\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Define variables\n",
    "        train = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(train)\n",
    "        \n",
    "        # Calculate softmax and cross entropy loss\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        # Prediction\n",
    "        if count % 50 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Predict test dataset\n",
    "            for images, labels in test_loader: \n",
    "                test = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(test)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += len(labels)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "        if count % 500 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))\n",
    "\n",
    "# visualization\n",
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Logistic Regression: Loss vs Number of iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b058c7e",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "### Artificial Neural Network (ANN)\n",
    "- Logistic regression is good at classification but when complexity(non linearity) increases, the accuracy of model decreases.\n",
    "- Therefore, we need to increase complexity of model.\n",
    "- In order to increase complexity of model, we need to add more non linear functions as hidden layer. \n",
    "- I am saying again that if you do not know what is artificial neural network check my deep learning tutorial because I will not explain neural network detailed here, only explain pytorch.\n",
    "- Artificial Neural Network tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n",
    "- What we expect from artificial neural network is that when complexity increases, we use more hidden layers and our model can adapt better. As a result accuracy increase.\n",
    "- **Steps of ANN:**\n",
    "    1. Import Libraries\n",
    "        - In order to show you, I import again but we actually imported them at previous parts.\n",
    "    1. Prepare Dataset\n",
    "        - Totally same with previous part(logistic regression).\n",
    "        - We use same dataset so we only need train_loader and test_loader. \n",
    "        - We use same batch size, epoch and iteration numbers.\n",
    "    1. Create ANN Model\n",
    "        - We add 3 hidden layers.\n",
    "        - We use ReLU, Tanh and ELU activation functions for diversity.\n",
    "    1. Instantiate Model Class\n",
    "        - input_dim = 28*28 # size of image px*px\n",
    "        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n",
    "        - Hidden layer dimension is 150. I only choose it as 150 there is no reason. Actually hidden layer dimension is hyperparameter and it should be chosen and tuned. You can try different values for hidden layer dimension and observe the results.\n",
    "        - create model\n",
    "    1. Instantiate Loss\n",
    "        - Cross entropy loss\n",
    "        - It also has softmax(logistic function) in it.\n",
    "    1. Instantiate Optimizer\n",
    "        - SGD Optimizer\n",
    "    1. Traning the Model\n",
    "    1. Prediction\n",
    "- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n",
    "- Thanks to hidden layers model learnt better and accuracy(almost 95%) is better than accuracy of logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0373494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create ANN Model\n",
    "class ANNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        \n",
    "        # Linear function 1: 784 --> 150\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 2: 150 --> 150\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        \n",
    "        # Linear function 3: 150 --> 150\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.elu3 = nn.ELU()\n",
    "        \n",
    "        # Linear function 4 (readout): 150 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.tanh2(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.elu3(out)\n",
    "        \n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "# instantiate ANN\n",
    "input_dim = 28*28\n",
    "hidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\n",
    "output_dim = 10\n",
    "\n",
    "# Create ANN\n",
    "model = ANNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Cross Entropy Loss \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ANN model training\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        train = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(train)\n",
    "        \n",
    "        # Calculate softmax and ross entropy loss\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if count % 50 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Predict test dataset\n",
    "            for images, labels in test_loader:\n",
    "\n",
    "                test = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(test)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += len(labels)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        if count % 500 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))\n",
    "\n",
    "# visualization loss \n",
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"ANN: Loss vs Number of iteration\")\n",
    "plt.show()\n",
    "\n",
    "# visualization accuracy \n",
    "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"ANN: Accuracy vs Number of iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "### Convolutional Neural Network (CNN)\n",
    "- CNN is well adapted to classify images.\n",
    "- You can learn CNN basics: https://www.kaggle.com/kanncaa1/convolutional-neural-network-cnn-tutorial\n",
    "- **Steps of CNN:**\n",
    "    1. Import Libraries\n",
    "    1. Prepare Dataset\n",
    "        - Totally same with previous parts.\n",
    "        - We use same dataset so we only need train_loader and test_loader. \n",
    "    1. Convolutional layer: \n",
    "        - Create feature maps with filters(kernels).\n",
    "        - Padding: After applying filter, dimensions of original image decreases. However, we want to preserve as much as information about the original image. We can apply padding to increase dimension of feature map after convolutional layer.\n",
    "        - We use 2 convolutional layer.\n",
    "        - Number of feature map is out_channels = 16\n",
    "        - Filter(kernel) size is 5*5\n",
    "    1. Pooling layer: \n",
    "        - Prepares a condensed feature map from output of convolutional layer(feature map) \n",
    "        - 2 pooling layer that we will use max pooling.\n",
    "        - Pooling size is 2*2\n",
    "    1. Flattening: Flats the features map\n",
    "    1. Fully Connected Layer: \n",
    "        - Artificial Neural Network that we learnt at previous part.\n",
    "        - Or it can be only linear like logistic regression but at the end there is always softmax function.\n",
    "        - We will not use activation function in fully connected layer.\n",
    "        - You can think that our fully connected layer is logistic regression.\n",
    "        - We combine convolutional part and logistic regression to create our CNN model.\n",
    "    1. Instantiate Model Class\n",
    "        - create model\n",
    "    1. Instantiate Loss\n",
    "        - Cross entropy loss\n",
    "        - It also has softmax(logistic function) in it.\n",
    "    1. Instantiate Optimizer\n",
    "        - SGD Optimizer\n",
    "    1. Traning the Model\n",
    "    1. Prediction\n",
    "- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n",
    "- Thanks to convolutional layer, model learnt better and accuracy(almost 98%) is better than accuracy of ANN. Actually while tuning hyperparameters, increase in iteration and expanding convolutional neural network can increase accuracy but it takes too much running time that we do not want at kaggle.   \n",
    "        \n",
    "\n",
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create CNN Model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "     \n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected 1\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "        # flatten\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# batch_size, epoch and iteration\n",
    "batch_size = 100\n",
    "n_iters = 2500\n",
    "num_epochs = n_iters / (len(features_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# Create CNN\n",
    "model = CNNModel()\n",
    "\n",
    "# Cross Entropy Loss \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# CNN model training\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        train = Variable(images.view(100,1,28,28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(train)\n",
    "        \n",
    "        # Calculate softmax and ross entropy loss\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if count % 50 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                \n",
    "                test = Variable(images.view(100,1,28,28))\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(test)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += len(labels)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        if count % 500 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))\n",
    "\n",
    "# visualization loss \n",
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CNN: Loss vs Number of iteration\")\n",
    "plt.show()\n",
    "\n",
    "# visualization accuracy \n",
    "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
    "plt.show()\n",
    "\n",
    "### Conclusion\n",
    "In this tutorial, we learn: \n",
    "1. Basics of pytorch\n",
    "1. Linear regression with pytorch\n",
    "1. Logistic regression with pytorch\n",
    "1. Artificial neural network with with pytorch\n",
    "1. Convolutional neural network with pytorch\n",
    "1. Recurrent neural network with pytorch\n",
    "    - https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch\n",
    "1. Long-Short Term Memory (LSTM)\n",
    "    - https://www.kaggle.com/kanncaa1/long-short-term-memory-with-pytorch\n",
    "\n",
    "<br> **If you have any question or suggest, I will be happy to hear it **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
