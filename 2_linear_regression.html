
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Linear Regression &#8212; Introduction to Engineering</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3. Logistic Regression" href="3_Logistic_regression.html" />
    <link rel="prev" title="1. Introduction to Machine Learning" href="1_Introduction_to_Machine_Learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Engineering</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to Introduction to Engineering
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_Introduction_to_Machine_Learning.html">
   1. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_Logistic_regression.html">
   3. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_KNN.html">
   4. K nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5_SVM.html">
   5. Support Vector Machine
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6_Decision_trees_and_random_forests.html">
   6. Decision Trees &amp; Random Forests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7_Bayesian_methods.html">
   7. Naive Bayesian Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8_k_means.html">
   8. K-Means
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9_Principal_Component_Analysis.html">
   9. Principal Component Analysis
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/2_linear_regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/Fangli-Ying/Introduction2Engineering/master?urlpath=tree/2_linear_regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        <a class="jupyterhub-button" href="https://jupyter.org/hub/hub/user-redirect/git-pull?repo=https://github.com/Fangli-Ying/Introduction2Engineering&urlpath=tree/Introduction2Engineering/2_linear_regression.ipynb&branch=master"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/Fangli-Ying/Introduction2Engineering/blob/master/2_linear_regression.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression">
     Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-in-a-nutshell">
     Linear regression in a nutshell
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problem-formulation">
     Problem formulation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-linear-regression">
     Simple Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-performance">
     Regression Performance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-using-a-linear-model-to-predict-country-happiness">
     Example: using a linear model to predict country happiness
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-loading-and-analysis">
     Data loading and analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#univariate-regression">
     Univariate regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#environment-setup">
   Environment setup
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-predict-country-happiness-using-only-gdp">
     Example: predict country happiness using only GDP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-splitting">
     Data splitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-plotting">
     Data plotting
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analytical-approach-normal-equation">
   Analytical approach: normal equation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Problem formulation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-function">
     Loss function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#analytical-solution">
     Analytical solution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#math-proof-optional">
     Math proof (optional)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vectorized-notation">
       Vectorized notation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vectorized-loss">
       Vectorized loss
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#loss-gradient">
       Loss gradient
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computation-of-loss-gradient-terms">
       Computation of loss gradient terms
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#final-gradient-expression">
       Final gradient expression
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#loss-minimization">
       Loss minimization
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-applying-normal-equation-to-predict-country-happiness">
     Example: applying Normal Equation to predict country happiness
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariate-regression">
     Multivariate regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pros-cons-of-analytical-approach">
     Pros/cons of analytical approach
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iterative-approach-gradient-descent">
   Iterative approach: gradient descent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#method-description">
     Method description
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-of-gradients">
     Computation of gradients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameters-update">
     Parameters update
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interactive-example">
     Interactive example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-types">
     Gradient descent types
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-applying-stochastic-gradient-descent-to-predict-country-happiness">
     Example: applying stochastic gradient descent to predict country happiness
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pros-cons-of-iterative-approach">
     Pros/cons of iterative approach
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#polynomial-regression">
   Polynomial Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-idea">
     General idea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-fitting-a-quadratic-curve-with-polynomial-regression">
     Example: fitting a quadratic curve with polynomial regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization">
   Regularization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     General idea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-observing-the-effects-of-regularization-rate">
     Example: observing the effects of regularization rate
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>2. Linear Regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression">
     Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-in-a-nutshell">
     Linear regression in a nutshell
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problem-formulation">
     Problem formulation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-linear-regression">
     Simple Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-performance">
     Regression Performance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-using-a-linear-model-to-predict-country-happiness">
     Example: using a linear model to predict country happiness
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-loading-and-analysis">
     Data loading and analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#univariate-regression">
     Univariate regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#environment-setup">
   Environment setup
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-predict-country-happiness-using-only-gdp">
     Example: predict country happiness using only GDP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-splitting">
     Data splitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-plotting">
     Data plotting
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analytical-approach-normal-equation">
   Analytical approach: normal equation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Problem formulation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-function">
     Loss function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#analytical-solution">
     Analytical solution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#math-proof-optional">
     Math proof (optional)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vectorized-notation">
       Vectorized notation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vectorized-loss">
       Vectorized loss
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#loss-gradient">
       Loss gradient
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computation-of-loss-gradient-terms">
       Computation of loss gradient terms
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#final-gradient-expression">
       Final gradient expression
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#loss-minimization">
       Loss minimization
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-applying-normal-equation-to-predict-country-happiness">
     Example: applying Normal Equation to predict country happiness
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariate-regression">
     Multivariate regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pros-cons-of-analytical-approach">
     Pros/cons of analytical approach
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iterative-approach-gradient-descent">
   Iterative approach: gradient descent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#method-description">
     Method description
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-of-gradients">
     Computation of gradients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameters-update">
     Parameters update
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interactive-example">
     Interactive example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-types">
     Gradient descent types
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-applying-stochastic-gradient-descent-to-predict-country-happiness">
     Example: applying stochastic gradient descent to predict country happiness
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pros-cons-of-iterative-approach">
     Pros/cons of iterative approach
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#polynomial-regression">
   Polynomial Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-idea">
     General idea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-fitting-a-quadratic-curve-with-polynomial-regression">
     Example: fitting a quadratic curve with polynomial regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization">
   Regularization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     General idea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-observing-the-effects-of-regularization-rate">
     Example: observing the effects of regularization rate
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="linear-regression">
<h1>2. Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">Â¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">Â¶</a></h2>
<p>Youâ€™re living in an era of large amounts of <strong>data, powerful computers, and artificial intelligence</strong>. This is just the beginning. Data science and machine learning are driving image recognition, development of autonomous vehicles, decisions in the financial and energy sectors, advances in medicine, the rise of social networks, and more. Linear regression is an important part of this.</p>
<p>Linear regression is one of the fundamental statistical and machine learning techniques. Whether you want to do statistics, machine learning, or scientific computing, thereâ€™s a good chance that youâ€™ll need it. Itâ€™s best to build a solid foundation first and then proceed toward more complex methods.</p>
<p>By the end of this article, youâ€™ll have learned:</p>
<ul class="simple">
<li><p>What linear regression is</p></li>
<li><p>What linear regression is used for</p></li>
<li><p>How linear regression works</p></li>
<li><p>How to implement linear regression in Python, step by step</p></li>
</ul>
<div class="section" id="regression">
<h3>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">Â¶</a></h3>
<p>Regression analysis is one of the most important fields in statistics and machine learning. There are many regression methods available. Linear regression is one of them.</p>
<p>Regression searches for relationships among <strong>variables</strong>. For example, you can observe several employees of some company and try to understand how their salaries depend on their <strong>features</strong>, such as experience, education level, role, city of employment, and so on.</p>
<p>This is a regression problem where data related to each employee represents one <strong>observation</strong>. The presumption is that the experience, education, role, and city are the independent features, while the salary depends on them.</p>
<p>Similarly, you can try to establish the mathematical dependence of housing prices on area, number of bedrooms, distance to the city center, and so on.</p>
<p>Generally, in regression analysis, you consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that at least one of the features depends on the others, you try to establish a relation among them.</p>
<p>In other words, you need to find a function that maps some features or variables to others sufficiently well.</p>
<p>The dependent features are called the <strong>dependent variables, outputs, or responses</strong>. The independent features are called the <strong>independent variables, inputs, regressors, or predictors</strong>.</p>
<p>Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, or brand.</p>
<p>Itâ€™s a common practice to denote the outputs with <span class="math notranslate nohighlight">\(ğ‘¦\)</span> and the inputs with ğ‘¥. If there are two or more independent variables, then they can be represented as the vector <span class="math notranslate nohighlight">\(ğ±\)</span> = (ğ‘¥â‚, â€¦, ğ‘¥áµ£), where ğ‘Ÿ is the number of inputs.</p>
</div>
<div class="section" id="linear-regression-in-a-nutshell">
<h3>Linear regression in a nutshell<a class="headerlink" href="#linear-regression-in-a-nutshell" title="Permalink to this headline">Â¶</a></h3>
<p>A linear regression model searches for a linear relationship between inputs (features) and output (target).
<img alt="LR" src="_images/linear_regression.png" /></p>
<ul class="simple">
<li><p>Video: <a class="reference external" href="https://www.youtube.com/watch?v=m88h75F3Rl8">Introduction to Linear Regression</a></p></li>
<li><p>Tutorial <a class="reference external" href="https://realpython.com/linear-regression-in-python/">Introduction to Linear Regression</a></p></li>
</ul>
</div>
<div class="section" id="problem-formulation">
<h3>Problem formulation<a class="headerlink" href="#problem-formulation" title="Permalink to this headline">Â¶</a></h3>
<p><strong>Math Explained</strong></p>
<p>When implementing linear regression of some dependent variable ğ‘¦ on the set of independent variables ğ± = (ğ‘¥â‚, â€¦, ğ‘¥áµ£), where ğ‘Ÿ is the number of predictors, you assume a linear relationship between ğ‘¦ and ğ±: ğ‘¦ = ğ›½â‚€ + ğ›½â‚ğ‘¥â‚ + â‹¯ + ğ›½áµ£ğ‘¥áµ£ + ğœ€. This equation is the <strong>regression equation</strong>. ğ›½â‚€, ğ›½â‚, â€¦, ğ›½áµ£ are the <strong>regression coefficients</strong>, and ğœ€ is the <strong>random error</strong>.</p>
<p>Linear regression calculates the <strong>estimators</strong> of the regression coefficients or simply the <strong>predicted weights</strong>, denoted with ğ‘â‚€, ğ‘â‚, â€¦, ğ‘áµ£. These estimators define the <strong>estimated regression function</strong> ğ‘“(ğ±) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ + ğ‘áµ£ğ‘¥áµ£. This function should capture the dependencies between the inputs and output sufficiently well.</p>
<p><strong>The estimated or predicted response</strong>, ğ‘“(ğ±áµ¢), for each observation ğ‘– = 1, â€¦, ğ‘›, should be as close as possible to the corresponding <strong>actual response</strong> ğ‘¦áµ¢. The differences ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢) for all observations ğ‘– = 1, â€¦, ğ‘›, are called the <strong>residuals</strong>. Regression is about determining <strong>the best predicted weights</strong>â€”that is, the weights corresponding to the smallest residuals.</p>
<p>To get the best weights, you usually <strong>minimize the sum of squared residuals (SSR)</strong> for all observations ğ‘– = 1, â€¦, ğ‘›: SSR = Î£áµ¢(ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢))Â². This approach is called the method of <strong>ordinary least squares</strong>.</p>
<p><strong>Machine Learning Perspective</strong></p>
<p>A linear model makes a prediction by computing a weighted sum of the input features, plus a constant term called <em>bias</em> (or sometimes <em>intercept</em>).</p>
<div class="math notranslate nohighlight">
\[y' = \theta_0 + \theta_1x_1 + \dotsc + \theta_nx_n\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y'\)</span> is the modelâ€™s output (prediction).</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of features for a data sample.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i, i \in [1..n]\)</span> is the value of the <span class="math notranslate nohighlight">\(i\)</span>th feature.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_i, i \in [0..n]\)</span> is <span class="math notranslate nohighlight">\(i\)</span>th model parameter. <span class="math notranslate nohighlight">\(\theta_0\)</span> is the bias term.</p></li>
</ul>
<p>The goal is to find the best set of parameters.</p>
<ul class="simple">
<li><p>Video: <a class="reference external" href="https://www.youtube.com/watch?v=erfeZg27B7A">Linear Regression Model</a></p></li>
</ul>
</div>
<div class="section" id="simple-linear-regression">
<h3>Simple Linear Regression<a class="headerlink" href="#simple-linear-regression" title="Permalink to this headline">Â¶</a></h3>
<p>Simple or single-variate linear regression is the simplest case of linear regression, as it has a single independent variable, ğ± = ğ‘¥.</p>
<p>The following figure illustrates simple linear regression:</p>
<p><img alt="LR" src="_images/LR.PNG" /></p>
<p>When implementing simple linear regression, you typically start with a given set of input-output (ğ‘¥-ğ‘¦) pairs. These pairs are your observations, shown as green circles in the figure. For example, the leftmost observation has the input ğ‘¥ = 5 and the actual output, or response, ğ‘¦ = 5. The next one has ğ‘¥ = 15 and ğ‘¦ = 20, and so on.</p>
<p>The estimated regression function, represented by the black line, has the equation ğ‘“(ğ‘¥) = ğ‘â‚€ + ğ‘â‚ğ‘¥. Your goal is to calculate the optimal values of the predicted weights ğ‘â‚€ and ğ‘â‚ that minimize SSR and determine the estimated regression function.</p>
<p>The value of ğ‘â‚€, also called the <strong>intercept</strong>, shows the point where the estimated regression line crosses the ğ‘¦ axis. Itâ€™s the value of the estimated response ğ‘“(ğ‘¥) for ğ‘¥ = 0. The value of ğ‘â‚ determines the <strong>slope</strong> of the estimated regression line.</p>
<p>The predicted responses, shown as red squares, are the points on the regression line that correspond to the input values. For example, for the input ğ‘¥ = 5, the predicted response is ğ‘“(5) = 8.33, which the leftmost red square represents.</p>
<p>The vertical dashed grey lines represent the residuals, which can be calculated as ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢) = ğ‘¦áµ¢ - ğ‘â‚€ - ğ‘â‚ğ‘¥áµ¢ for ğ‘– = 1, â€¦, ğ‘›. Theyâ€™re the distances between the green circles and red squares. When you implement linear regression, youâ€™re actually trying to minimize these distances and make the red squares as close to the predefined green circles as possible.</p>
</div>
<div class="section" id="regression-performance">
<h3>Regression Performance<a class="headerlink" href="#regression-performance" title="Permalink to this headline">Â¶</a></h3>
<p>The variation of actual responses ğ‘¦áµ¢, ğ‘– = 1, â€¦, ğ‘›, occurs partly due to the dependence on the predictors ğ±áµ¢. However, thereâ€™s also an additional inherent variance of the output.</p>
<p><strong>The coefficient of determination</strong>, denoted as ğ‘…Â², tells you which amount of variation in ğ‘¦ can be explained by the dependence on ğ±, using the particular regression model. A larger ğ‘…Â² indicates a better fit and means that the model can better explain the variation of the output with different inputs.</p>
<p>The value ğ‘…Â² = 1 corresponds to SSR = 0. Thatâ€™s the <strong>perfect fit</strong>, since the values of predicted and actual responses fit completely to each other.</p>
</div>
<div class="section" id="example-using-a-linear-model-to-predict-country-happiness">
<h3>Example: using a linear model to predict country happiness<a class="headerlink" href="#example-using-a-linear-model-to-predict-country-happiness" title="Permalink to this headline">Â¶</a></h3>
<p>(Inspired by <a class="reference external" href="https://github.com/trekhleb/homemade-machine-learning">Homemade Machine Learning</a> by Oleksii Trekhleb)</p>
<p>The <a class="reference external" href="https://www.kaggle.com/unsdsn/world-happiness#2017.csv">World Happiness Report</a> ranks 155 countries by their happiness levels. Several economic and social indicators (GDP, degree of freedom, level of corruptionâ€¦) are recorded for each country.</p>
<p>Can a linear model accurately predict country happiness based on these indicators ?</p>
</div>
<div class="section" id="data-loading-and-analysis">
<h3>Data loading and analysis<a class="headerlink" href="#data-loading-and-analysis" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load World Happiness Report for 2017</span>
<span class="n">dataset_url</span> <span class="o">=</span> <span class="s2">&quot;./data/2017.csv&quot;</span>
<span class="n">df_happiness</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">dataset_url</span><span class="p">)</span>

<span class="c1"># Print dataset shape (rows and columns)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset shape: </span><span class="si">{</span><span class="n">df_happiness</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="mf">4e05</span><span class="n">e6bded9f</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># Load World Happiness Report for 2017</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">dataset_url</span> <span class="o">=</span> <span class="s2">&quot;./data/2017.csv&quot;</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">df_happiness</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">dataset_url</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> 
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="c1"># Print dataset shape (rows and columns)</span>

<span class="ne">NameError</span>: name &#39;pd&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print a concise summary of the dataset</span>
<span class="n">df_happiness</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 155 entries, 0 to 154
Data columns (total 12 columns):
 #   Column                         Non-Null Count  Dtype  
---  ------                         --------------  -----  
 0   Country                        155 non-null    object 
 1   Happiness.Rank                 155 non-null    int64  
 2   Happiness.Score                155 non-null    float64
 3   Whisker.high                   155 non-null    float64
 4   Whisker.low                    155 non-null    float64
 5   Economy..GDP.per.Capita.       155 non-null    float64
 6   Family                         155 non-null    float64
 7   Health..Life.Expectancy.       155 non-null    float64
 8   Freedom                        155 non-null    float64
 9   Generosity                     155 non-null    float64
 10  Trust..Government.Corruption.  155 non-null    float64
 11  Dystopia.Residual              155 non-null    float64
dtypes: float64(10), int64(1), object(1)
memory usage: 14.7+ KB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show the 10 first samples</span>
<span class="n">df_happiness</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Country</th>
      <th>Happiness.Rank</th>
      <th>Happiness.Score</th>
      <th>Whisker.high</th>
      <th>Whisker.low</th>
      <th>Economy..GDP.per.Capita.</th>
      <th>Family</th>
      <th>Health..Life.Expectancy.</th>
      <th>Freedom</th>
      <th>Generosity</th>
      <th>Trust..Government.Corruption.</th>
      <th>Dystopia.Residual</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Norway</td>
      <td>1</td>
      <td>7.537</td>
      <td>7.594445</td>
      <td>7.479556</td>
      <td>1.616463</td>
      <td>1.533524</td>
      <td>0.796667</td>
      <td>0.635423</td>
      <td>0.362012</td>
      <td>0.315964</td>
      <td>2.277027</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Denmark</td>
      <td>2</td>
      <td>7.522</td>
      <td>7.581728</td>
      <td>7.462272</td>
      <td>1.482383</td>
      <td>1.551122</td>
      <td>0.792566</td>
      <td>0.626007</td>
      <td>0.355280</td>
      <td>0.400770</td>
      <td>2.313707</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Iceland</td>
      <td>3</td>
      <td>7.504</td>
      <td>7.622030</td>
      <td>7.385970</td>
      <td>1.480633</td>
      <td>1.610574</td>
      <td>0.833552</td>
      <td>0.627163</td>
      <td>0.475540</td>
      <td>0.153527</td>
      <td>2.322715</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Switzerland</td>
      <td>4</td>
      <td>7.494</td>
      <td>7.561772</td>
      <td>7.426227</td>
      <td>1.564980</td>
      <td>1.516912</td>
      <td>0.858131</td>
      <td>0.620071</td>
      <td>0.290549</td>
      <td>0.367007</td>
      <td>2.276716</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Finland</td>
      <td>5</td>
      <td>7.469</td>
      <td>7.527542</td>
      <td>7.410458</td>
      <td>1.443572</td>
      <td>1.540247</td>
      <td>0.809158</td>
      <td>0.617951</td>
      <td>0.245483</td>
      <td>0.382612</td>
      <td>2.430182</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Netherlands</td>
      <td>6</td>
      <td>7.377</td>
      <td>7.427426</td>
      <td>7.326574</td>
      <td>1.503945</td>
      <td>1.428939</td>
      <td>0.810696</td>
      <td>0.585384</td>
      <td>0.470490</td>
      <td>0.282662</td>
      <td>2.294804</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Canada</td>
      <td>7</td>
      <td>7.316</td>
      <td>7.384403</td>
      <td>7.247597</td>
      <td>1.479204</td>
      <td>1.481349</td>
      <td>0.834558</td>
      <td>0.611101</td>
      <td>0.435540</td>
      <td>0.287372</td>
      <td>2.187264</td>
    </tr>
    <tr>
      <th>7</th>
      <td>New Zealand</td>
      <td>8</td>
      <td>7.314</td>
      <td>7.379510</td>
      <td>7.248490</td>
      <td>1.405706</td>
      <td>1.548195</td>
      <td>0.816760</td>
      <td>0.614062</td>
      <td>0.500005</td>
      <td>0.382817</td>
      <td>2.046456</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Sweden</td>
      <td>9</td>
      <td>7.284</td>
      <td>7.344095</td>
      <td>7.223905</td>
      <td>1.494387</td>
      <td>1.478162</td>
      <td>0.830875</td>
      <td>0.612924</td>
      <td>0.385399</td>
      <td>0.384399</td>
      <td>2.097538</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Australia</td>
      <td>10</td>
      <td>7.284</td>
      <td>7.356651</td>
      <td>7.211349</td>
      <td>1.484415</td>
      <td>1.510042</td>
      <td>0.843887</td>
      <td>0.601607</td>
      <td>0.477699</td>
      <td>0.301184</td>
      <td>2.065211</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot histograms for all numerical attributes</span>
<span class="n">df_happiness</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_linear_regression_19_0.png" src="_images/2_linear_regression_19_0.png" />
</div>
</div>
</div>
<div class="section" id="univariate-regression">
<h3>Univariate regression<a class="headerlink" href="#univariate-regression" title="Permalink to this headline">Â¶</a></h3>
<p>Only one feature is used by the model, which has two parameters.</p>
<div class="math notranslate nohighlight">
\[y' = \theta_0 + \theta_1x\]</div>
</div>
</div>
<div class="section" id="environment-setup">
<h2>Environment setup<a class="headerlink" href="#environment-setup" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">platform</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Python version: </span><span class="si">{</span><span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">platform</span><span class="o">.</span><span class="n">python_version_tuple</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="s2">&quot;3&quot;</span><span class="p">,</span> <span class="s2">&quot;6&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">plotly</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objs</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;seaborn version: </span><span class="si">{</span><span class="n">sns</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup plots</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">8</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &quot;retina&quot;
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>

<span class="c1"># Configure Plotly to be rendered inline in the notebook.</span>
<span class="n">plotly</span><span class="o">.</span><span class="n">offline</span><span class="o">.</span><span class="n">init_notebook_mode</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span> 
<span class="kn">import</span> <span class="nn">sklearn</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;scikit-learn version: </span><span class="si">{</span><span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&gt;=</span> <span class="s2">&quot;0.20&quot;</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">SGDRegressor</span><span class="p">,</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="example-predict-country-happiness-using-only-gdp">
<h3>Example: predict country happiness using only GDP<a class="headerlink" href="#example-predict-country-happiness-using-only-gdp" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">filter_dataset</span><span class="p">(</span><span class="n">df_data</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">target_feature</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return a dataset containing only the selected input and output features&quot;&quot;&quot;</span>
    
    <span class="n">feature_list</span> <span class="o">=</span> <span class="n">input_features</span> <span class="o">+</span> <span class="p">[</span><span class="n">target_feature</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">df_data</span><span class="p">[</span><span class="n">feature_list</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define GDP as sole input feature</span>
<span class="n">input_features_uni</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Economy..GDP.per.Capita.&quot;</span><span class="p">]</span>
<span class="c1"># Define country happiness as target</span>
<span class="n">target_feature</span> <span class="o">=</span> <span class="s2">&quot;Happiness.Score&quot;</span>

<span class="n">df_happiness_uni</span> <span class="o">=</span> <span class="n">filter_dataset</span><span class="p">(</span><span class="n">df_happiness</span><span class="p">,</span> <span class="n">input_features_uni</span><span class="p">,</span> <span class="n">target_feature</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show 10 random samples</span>
<span class="n">df_happiness_uni</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Economy..GDP.per.Capita.</th>
      <th>Happiness.Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>145</th>
      <td>0.591683</td>
      <td>3.593</td>
    </tr>
    <tr>
      <th>57</th>
      <td>0.833757</td>
      <td>5.823</td>
    </tr>
    <tr>
      <th>151</th>
      <td>0.777153</td>
      <td>3.462</td>
    </tr>
    <tr>
      <th>131</th>
      <td>0.894652</td>
      <td>4.096</td>
    </tr>
    <tr>
      <th>56</th>
      <td>1.217684</td>
      <td>5.825</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0.234306</td>
      <td>4.550</td>
    </tr>
    <tr>
      <th>29</th>
      <td>1.233748</td>
      <td>6.452</td>
    </tr>
    <tr>
      <th>94</th>
      <td>0.783756</td>
      <td>5.074</td>
    </tr>
    <tr>
      <th>72</th>
      <td>1.069318</td>
      <td>5.395</td>
    </tr>
    <tr>
      <th>31</th>
      <td>1.127869</td>
      <td>6.424</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="data-splitting">
<h3>Data splitting<a class="headerlink" href="#data-splitting" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">split_dataset</span><span class="p">(</span><span class="n">df_data</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">target_feature</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Split dataset between training and test sets, keeping only selected features&quot;&quot;&quot;</span>
    
    <span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df_data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training dataset: </span><span class="si">{</span><span class="n">df_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test dataset: </span><span class="si">{</span><span class="n">df_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">x_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="n">input_features</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="n">target_feature</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

    <span class="n">x_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="n">input_features</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="n">target_feature</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training data: </span><span class="si">{</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, labels: </span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test data: </span><span class="si">{</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, labels: </span><span class="si">{</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train_uni</span><span class="p">,</span> <span class="n">y_train_uni</span><span class="p">,</span> <span class="n">x_test_uni</span><span class="p">,</span> <span class="n">y_test_uni</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">(</span>
    <span class="n">df_happiness_uni</span><span class="p">,</span> <span class="n">input_features_uni</span><span class="p">,</span> <span class="n">target_feature</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training dataset: (124, 2)
Test dataset: (31, 2)
Training data: (124, 1), labels: (124,)
Test data: (31, 1), labels: (31,)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data-plotting">
<h3>Data plotting<a class="headerlink" href="#data-plotting" title="Permalink to this headline">Â¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_univariate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">target_features</span><span class="p">,</span> <span class="n">model_list</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;2D plot of features and target, including model prediction if defined&quot;&quot;&quot;</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">predictions_count</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">predictions_count</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">predictions_count</span><span class="p">,</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">model_list</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">target_feature</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Countries Happiness&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot training data</span>
<span class="n">plot_univariate</span><span class="p">(</span><span class="n">x_train_uni</span><span class="p">,</span> <span class="n">y_train_uni</span><span class="p">,</span> <span class="n">input_features_uni</span><span class="p">,</span> <span class="n">target_feature</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_linear_regression_34_0.png" src="_images/2_linear_regression_34_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="analytical-approach-normal-equation">
<h2>Analytical approach: normal equation<a class="headerlink" href="#analytical-approach-normal-equation" title="Permalink to this headline">Â¶</a></h2>
<ul class="simple">
<li><p>Video: <a class="reference external" href="https://www.youtube.com/watch?v=Qa_FI92_qo8">Linear Regression in Matrix</a></p></li>
</ul>
<div class="section" id="id1">
<h3>Problem formulation<a class="headerlink" href="#id1" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\pmb{x}^{(i)}\)</span>: <span class="math notranslate nohighlight">\(i\)</span>th data sample, vector of <span class="math notranslate nohighlight">\(n+1\)</span> features <span class="math notranslate nohighlight">\(x^{(i)}_j\)</span> with <span class="math notranslate nohighlight">\(x^{(i)}_0 = 1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\pmb{\theta}\)</span>: parameters of the linear model, vector of <span class="math notranslate nohighlight">\(n+1\)</span> values <span class="math notranslate nohighlight">\(\theta_j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{h}_\theta\)</span>: hypothesis function (relationship between inputs and targets).</p></li>
<li><p><span class="math notranslate nohighlight">\(y'^{(i)}\)</span>: model output for the <span class="math notranslate nohighlight">\(i\)</span>th sample.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\pmb{x}^{(i)} = \begin{pmatrix}
       \ x^{(i)}_0 \\
       \ x^{(i)}_1 \\
       \ \vdots \\
       \ x^{(i)}_n
     \end{pmatrix} \in \pmb{R}^{n+1}
\;\;\;
\pmb{\theta} = \begin{pmatrix}
       \ \theta_0 \\
       \ \theta_1 \\
       \ \vdots \\
       \ \theta_n
     \end{pmatrix} \in \pmb{R}^{n+1}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[y'^{(i)} = \mathcal{h}_\theta(x^{(i)}) = \theta_0 + \theta_1x^{(i)}_1 + \dotsc + \theta_nx^{(i)}_n = \pmb{\theta}^T\pmb{x}^{(i)}\]</div>
</div>
<div class="section" id="loss-function">
<h3>Loss function<a class="headerlink" href="#loss-function" title="Permalink to this headline">Â¶</a></h3>
<p>We use the <em>Mean Squared Error</em> (MSE). RMSE is also a possible choice.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\pmb{\theta}) = \frac{1}{m}\sum_{i=1}^m (y'^{(i)} - y^{(i)})^2 = \frac{1}{m}\sum_{i=1}^m (\mathcal{h}_\theta(x^{(i)}) - y^{(i)})^2\]</div>
</div>
<div class="section" id="analytical-solution">
<h3>Analytical solution<a class="headerlink" href="#analytical-solution" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>Technique for computing the regression coefficients <span class="math notranslate nohighlight">\(\theta_i\)</span> analytically (by calculus).</p></li>
<li><p>One-step learning algorithm (no iterations).</p></li>
<li><p>Also called <em>Ordinary Least Squares</em>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\pmb{\theta^{*}} = (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb{y}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\pmb{\theta^*}\)</span> is the parameter vector that minimizes the loss function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>.</p></li>
<li><p>This result is called the <strong>Normal Equation</strong>.</p></li>
</ul>
</div>
<div class="section" id="math-proof-optional">
<h3>Math proof (optional)<a class="headerlink" href="#math-proof-optional" title="Permalink to this headline">Â¶</a></h3>
<p>(Inspired by <a class="reference external" href="https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/">Derivation of the Normal Equation for linear regression</a> and <a class="reference external" href="http://www.oranlooney.com/post/ml-from-scratch-part-1-linear-regression/">ML From Scratch, Part 1: Linear Regression</a>)</p>
<div class="section" id="vectorized-notation">
<h4>Vectorized notation<a class="headerlink" href="#vectorized-notation" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\pmb{X}\)</span>: matrix of input data (<em>design matrix</em>). Each line corresponds to a sample.</p></li>
<li><p><span class="math notranslate nohighlight">\(\pmb{y}\)</span>: vector of target values.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\pmb{X} = \begin{bmatrix}
       \ x^{(0)T} \\
       \ x^{(1)T} \\
       \ \vdots \\
       \ x^{(m)T} \\
     \end{bmatrix} =
\begin{bmatrix}
       \ x^{(1)}_0 &amp; x^{(1)}_1 &amp; \cdots &amp; x^{(1)}_n \\
       \ x^{(2)}_0 &amp; x^{(2)}_1 &amp; \cdots &amp; x^{(2)}_n \\
       \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
       \ x^{(m)}_0 &amp; x^{(m)}_1 &amp; \cdots &amp; x^{(m)}_n
     \end{bmatrix}\;\;\;
\pmb{y} = \begin{pmatrix}
       \ y^{(1)} \\
       \ y^{(2)} \\
       \ \vdots \\
       \ y^{(m)}
     \end{pmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\pmb{X}\pmb{\theta} =
\begin{pmatrix}
       \ \theta_0 + \theta_1x^{(1)}_1 + \dotsc + \theta_nx^{(1)}_n \\
       \ \theta_0 + \theta_1x^{(2)}_1 + \dotsc + \theta_nx^{(2)}_n \\
       \ \vdots \\
       \ \theta_0 + \theta_1x^{(m)}_1 + \dotsc + \theta_nx^{(m)}_n
     \end{pmatrix} = 
\begin{pmatrix}
       \ \mathcal{h}_\theta(x^{(1)}) \\
       \ \mathcal{h}_\theta(x^{(2)}) \\
       \ \vdots \\
       \ \mathcal{h}_\theta(x^{(m)})
     \end{pmatrix}\end{split}\]</div>
</div>
<div class="section" id="vectorized-loss">
<h4>Vectorized loss<a class="headerlink" href="#vectorized-loss" title="Permalink to this headline">Â¶</a></h4>
<p>The loss can also be expressed using a vectorized notation.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta) = \frac{1}{m}\sum_{i=1}^m (\mathcal{h}_\theta(x^{(i)}) - y^{(i)})^2 = \frac{1}{m}{{\lVert{\pmb{X}\pmb{\theta} - \pmb{y}}\rVert}_2}^2\]</div>
<p>The squared norm of a vector <span class="math notranslate nohighlight">\(\pmb{v}\)</span> is the inner product of that vector with its transpose: <span class="math notranslate nohighlight">\(\sum_{i=1}^n v_i^2 = \pmb{v}^T \pmb{v}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta) = \frac{1}{m}(\pmb{X}\pmb{\theta} - \pmb{y})^T(\pmb{X}\pmb{\theta} - \pmb{y})\]</div>
<p>The previous expression can be developped.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta) = \frac{1}{m}\left((\pmb{X}\pmb{\theta})^T - \pmb{y}^T)(\pmb{X}\pmb{\theta} - \pmb{y}\right) = \frac{1}{m}\left((\pmb{X}\pmb{\theta})^T\pmb{X}\pmb{\theta} - (\pmb{X}\pmb{\theta})^T\pmb{y} - \pmb{y}^T(\pmb{X}\pmb{\theta}) + \pmb{y}^T\pmb{y}\right)\]</div>
<p>Since <span class="math notranslate nohighlight">\(\pmb{X}\pmb{\theta}\)</span> and <span class="math notranslate nohighlight">\(\pmb{y}\)</span> are vectors, <span class="math notranslate nohighlight">\((\pmb{X}\pmb{\theta})^T\pmb{y} = \pmb{y}^T(\pmb{X}\pmb{\theta})\)</span>.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta) = \frac{1}{m}\left(\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta} - 2(\pmb{X}\pmb{\theta})^T\pmb{y} + \pmb{y}^T\pmb{y}\right)\]</div>
</div>
<div class="section" id="loss-gradient">
<h4>Loss gradient<a class="headerlink" href="#loss-gradient" title="Permalink to this headline">Â¶</a></h4>
<p>We must find the <span class="math notranslate nohighlight">\(\pmb{\theta^*}\)</span> vector that minimizes the loss function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\pmb{\theta^*} = \underset{\theta}{\mathrm{argmin}}\;\mathcal{L}(\theta)\]</div>
<p>Since the loss function is continuous, convex and differentiable everywhere (in simplest termes: bowl-shaped), it admits one unique global minimum, for which the gradient vector <span class="math notranslate nohighlight">\(\nabla_{\theta}\mathcal{L}(\pmb{\theta})\)</span> is equal to <span class="math notranslate nohighlight">\(\vec{0}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla_{\theta}\mathcal{L}(\pmb{\theta}) = \begin{pmatrix}
       \ \frac{\partial}{\partial \theta_0} \mathcal{L}(\boldsymbol{\theta}) \\
       \ \frac{\partial}{\partial \theta_1} \mathcal{L}(\boldsymbol{\theta}) \\
       \ \vdots \\
       \ \frac{\partial}{\partial \theta_n} \mathcal{L}(\boldsymbol{\theta})
     \end{pmatrix} = \nabla_{\theta}\left(\frac{1}{m}(\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta} - 2(\pmb{X}\pmb{\theta})^T\pmb{y} + \pmb{y}^T\pmb{y})\right)\end{split}\]</div>
</div>
<div class="section" id="computation-of-loss-gradient-terms">
<h4>Computation of loss gradient terms<a class="headerlink" href="#computation-of-loss-gradient-terms" title="Permalink to this headline">Â¶</a></h4>
<p>Since <span class="math notranslate nohighlight">\(\pmb{y}^T\pmb{y}\)</span> is constant w.r.t. <span class="math notranslate nohighlight">\(\pmb{\theta}\)</span>, <span class="math notranslate nohighlight">\(\nabla_{\theta}(\pmb{y}^T\pmb{y}) = \vec{0}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}2(\pmb{X}\pmb{\theta})^T\pmb{y} = 2\;\begin{pmatrix}
       \ \theta_0 + \theta_1x^{(1)}_1 + \dotsc + \theta_nx^{(1)}_n \\
       \ \theta_0 + \theta_1x^{(2)}_1 + \dotsc + \theta_nx^{(2)}_n \\
       \ \vdots \\
       \ \theta_0 + \theta_1x^{(m)}_1 + \dotsc + \theta_nx^{(m)}_n
     \end{pmatrix}^T\begin{pmatrix}
       \ y^{(1)} \\
       \ y^{(2)} \\
       \ \vdots \\
       \ y^{(m)}
     \end{pmatrix} = 2\sum_{i=1}^m y^{(i)}(\theta_0 + \theta_1x^{(i)}_1 + \dotsc + \theta_nx^{(i)}_n)\end{split}\]</div>
<p>Reminder: <span class="math notranslate nohighlight">\(\forall i \in [1..m], x_0^{(i)} = 1\)</span>.</p>
<div class="math notranslate nohighlight">
\[2(\pmb{X}\pmb{\theta})^T\pmb{y} =2\sum_{i=1}^m y^{(i)}\sum_{j=0}^n x_j^{(i)}\theta_j\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla_{\theta}\left(2(\pmb{X}\pmb{\theta})^T\pmb{y}\right) = 
\begin{pmatrix}
       \ \frac{\partial}{\partial \theta_0} \left(2(\pmb{X}\pmb{\theta})^T\pmb{y}\right)  \\
       \ \frac{\partial}{\partial \theta_1} \left(2(\pmb{X}\pmb{\theta})^T\pmb{y}\right) \\
       \ \vdots \\
       \ \frac{\partial}{\partial \theta_n} \left(2(\pmb{X}\pmb{\theta})^T\pmb{y}\right)
     \end{pmatrix} =
2\begin{pmatrix}
       \ \sum_{i=1}^m y^{(i)}x_0^{(i)} \\
       \ \sum_{i=1}^m y^{(i)}x_1^{(i)} \\
       \ \vdots \\
       \ \sum_{i=1}^m y^{(i)}x_n^{(i)}
     \end{pmatrix} = 2 \pmb{X}^T\pmb{y}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\pmb{X}^T\pmb{X}\)</span> is a square and symmetric matrix called <span class="math notranslate nohighlight">\(\pmb{A}\)</span> here for simplicity of notation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\pmb{X}^T\pmb{X} = \begin{bmatrix}
       \ x^{(1)}_0 &amp; x^{(2)}_0 &amp; \cdots &amp; x^{(m)}_0 \\
       \ x^{(1)}_1 &amp; x^{(2)}_1 &amp; \cdots &amp; x^{(m)}_1 \\
       \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
       \ x^{(1)}_n &amp; x^{(2)}_n &amp; \cdots &amp; x^{(m)}_n
     \end{bmatrix}
\begin{bmatrix}
       \ x^{(1)}_0 &amp; x^{(1)}_1 &amp; \cdots &amp; x^{(1)}_n \\
       \ x^{(2)}_0 &amp; x^{(2)}_1 &amp; \cdots &amp; x^{(2)}_n \\
       \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
       \ x^{(m)}_0 &amp; x^{(m)}_1 &amp; \cdots &amp; x^{(m)}_n
     \end{bmatrix} = \pmb{A} \in \pmb{R}^{n+1 \times n+1}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta} = \begin{pmatrix}
       \ \theta_0 \\
       \ \theta_1 \\
       \ \vdots \\
       \ \theta_n
     \end{pmatrix}^T
     \begin{bmatrix}
       \ a_{00} &amp; a_{01} &amp; \cdots &amp; a_{0n} \\
       \ a_{10} &amp; a_{11} &amp; \cdots &amp; a_{1n} \\
       \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
       \ a_{n0} &amp; a_{n1} &amp; \cdots &amp; a_{nn}
     \end{bmatrix}
     \begin{pmatrix}
       \ \theta_0 \\
       \ \theta_1 \\
       \ \vdots \\
       \ \theta_n
     \end{pmatrix} = 
     \begin{pmatrix}
       \ \theta_0 \\
       \ \theta_1 \\
       \ \vdots \\
       \ \theta_n
     \end{pmatrix}^T
     \begin{pmatrix}
       \ a_{00}\theta_0 + a_{01}\theta_1 + \dotsc + a_{0n}\theta_n \\
       \ a_{10}\theta_0 + a_{11}\theta_1 + \dotsc + a_{1n}\theta_n \\
       \ \vdots \\
       \ a_{n0}\theta_0 + a_{n1}\theta_1 + \dotsc + a_{nn}\theta_n
     \end{pmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta} = \theta_0(a_{00}\theta_0 + a_{01}\theta_1 + \dotsc + a_{0n}\theta_n) + \theta_1(a_{10}\theta_0 + a_{11}\theta_1 + \dotsc + a_{1n}\theta_n) + \dotsc + \theta_n(a_{n0}\theta_0 + a_{n1}\theta_1 + \dotsc + a_{nn}\theta_n)\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_0} \left(\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta}\right) =
2a_{00}\theta_0 + a_{01}\theta_1 + \dotsc + a_{0n}\theta_n + a_{10}\theta_1 + a_{20}\theta_2 + \dotsc + a_{n0}\theta_n\]</div>
<p>Since <span class="math notranslate nohighlight">\(\pmb{A}\)</span> is symmetric, <span class="math notranslate nohighlight">\(\forall i,j \in [1..n,1..n], a_{ij} = a_{ji}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_0} \left(\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta}\right) =
2(a_{00}\theta_0 + a_{01}\theta_1 + \dotsc + a_{0n}\theta_n) =
2\sum_{j=0}^n a_{0j}\theta_j\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla_{\theta}\left(\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta}\right)=
\begin{pmatrix}
       \ \frac{\partial}{\partial \theta_0} \left(\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta}\right)  \\
       \ \frac{\partial}{\partial \theta_1} \left(\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta}\right) \\
       \ \vdots \\
       \ \frac{\partial}{\partial \theta_n} \left(\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta}\right)
     \end{pmatrix} =
     2\begin{pmatrix}
       \ \sum_{j=0}^n a_{0j}\theta_j  \\
       \ \sum_{j=0}^n a_{1j}\theta_j \\
       \ \vdots \\
       \ \sum_{j=0}^n a_{nj}\theta_j
     \end{pmatrix}=
     2\pmb{A}\pmb{\theta} = 2\pmb{X}^T\pmb{X}\pmb{\theta}\end{split}\]</div>
</div>
<div class="section" id="final-gradient-expression">
<h4>Final gradient expression<a class="headerlink" href="#final-gradient-expression" title="Permalink to this headline">Â¶</a></h4>
<p>We can finally express the gradient of the loss function w.r.t. the model parameters:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta}\mathcal{L}(\pmb{\theta}) = \nabla_{\theta}\left(\frac{1}{m}(\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta} - 2(\pmb{X}\pmb{\theta})^T\pmb{y} + \pmb{y}^T\pmb{y})\right) = \frac{1}{m}\left(2\pmb{X}^T\pmb{X}\pmb{\theta} - 2\pmb{X}^T\pmb{y}\right)\]</div>
<div class="math notranslate nohighlight">
\[\nabla_{\theta}\mathcal{L}(\pmb{\theta}) = \frac{2}{m}\pmb{X}^T\left(\pmb{X}\pmb{\theta} - \pmb{y}\right)\]</div>
</div>
<div class="section" id="loss-minimization">
<h4>Loss minimization<a class="headerlink" href="#loss-minimization" title="Permalink to this headline">Â¶</a></h4>
<p>The <span class="math notranslate nohighlight">\(\pmb{\theta^*}\)</span> vector that minimizes the loss is such as the gradient is equal to <span class="math notranslate nohighlight">\(\vec{0}\)</span>. In other terms:</p>
<div class="math notranslate nohighlight">
\[\pmb{X}^T\pmb{X}\pmb{\theta^{*}} - \pmb{X}^T\pmb{y} = \vec{0}\]</div>
<div class="math notranslate nohighlight">
\[\pmb{X}^T\pmb{X}\pmb{\theta^{*}} = \pmb{X}^T\pmb{y}\]</div>
<p>If <span class="math notranslate nohighlight">\(\pmb{X}^T\pmb{X}\)</span> is an inversible matrix, the result is given by:</p>
<div class="math notranslate nohighlight">
\[\pmb{\theta^{*}} = (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb{y}\]</div>
<p>Which is exactly the Normal Equation we were expecting to see!</p>
<ul class="simple">
<li><p>tutorial <a class="reference external" href="https://muthu.co/math-behind-linear-regression-and-python-code/"> linear regression with example</a></p></li>
</ul>
</div>
</div>
<div class="section" id="example-applying-normal-equation-to-predict-country-happiness">
<h3>Example: applying Normal Equation to predict country happiness<a class="headerlink" href="#example-applying-normal-equation-to-predict-country-happiness" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="c1">#training model with all x y</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model weights: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="si">}</span><span class="s2">, bias: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">error</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training error: </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">.05f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">test_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test error: </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">.05f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Linear Regression model (based on Normal Equation)</span>
<span class="n">lr_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Train and test the model on univariate data</span>
<span class="n">train_model</span><span class="p">(</span><span class="n">lr_model</span><span class="p">,</span> <span class="n">x_train_uni</span><span class="p">,</span> <span class="n">y_train_uni</span><span class="p">)</span>
<span class="n">test_model</span><span class="p">(</span><span class="n">lr_model</span><span class="p">,</span> <span class="n">x_test_uni</span><span class="p">,</span> <span class="n">y_test_uni</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [2.11218002], bias: 3.2399565304753697
Training error: 0.41461
Test error: 0.51273
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot data and model prediction</span>
<span class="n">plot_univariate</span><span class="p">(</span>
    <span class="n">x_train_uni</span><span class="p">,</span>
    <span class="n">y_train_uni</span><span class="p">,</span>
    <span class="n">input_features_uni</span><span class="p">,</span>
    <span class="n">target_feature</span><span class="p">,</span>
    <span class="n">model_list</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;LR&quot;</span><span class="p">:</span> <span class="n">lr_model</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_linear_regression_56_0.png" src="_images/2_linear_regression_56_0.png" />
</div>
</div>
</div>
<div class="section" id="multivariate-regression">
<h3>Multivariate regression<a class="headerlink" href="#multivariate-regression" title="Permalink to this headline">Â¶</a></h3>
<p>General case: several features are used by the model.</p>
<div class="math notranslate nohighlight">
\[y' = \theta_0 + \theta_1x_1 + \dotsc + \theta_nx_n\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Using two input features: GDP and degree of freedom</span>
<span class="n">input_features_multi</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Economy..GDP.per.Capita.&quot;</span><span class="p">,</span> <span class="s2">&quot;Freedom&quot;</span><span class="p">]</span>

<span class="n">x_train_multi</span><span class="p">,</span> <span class="n">y_train_multi</span><span class="p">,</span> <span class="n">x_test_multi</span><span class="p">,</span> <span class="n">y_test_multi</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">(</span>
    <span class="n">df_happiness</span><span class="p">,</span> <span class="n">input_features_multi</span><span class="p">,</span> <span class="n">target_feature</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training dataset: (124, 12)
Test dataset: (31, 12)
Training data: (124, 2), labels: (124,)
Test data: (31, 2), labels: (31,)
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_multivariate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">target_features</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;3D plot of features and target, including model prediction if defined&quot;&quot;&quot;</span>
    
    <span class="c1"># Configure the plot with training dataset</span>
    <span class="n">plot_training_trace</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
        <span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
        <span class="n">z</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="n">marker</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
            <span class="s2">&quot;opacity&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;line&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;rgb(255, 255, 255)&quot;</span><span class="p">,</span> <span class="s2">&quot;width&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
        <span class="p">},</span>
    <span class="p">)</span>

    <span class="n">plot_data</span> <span class="o">=</span> <span class="n">plot_training_trace</span>

    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Generate different combinations of X and Y sets to build a predictions plane.</span>
        <span class="n">predictions_count</span> <span class="o">=</span> <span class="mi">10</span>

        <span class="c1"># Find min and max values along X and Y axes.</span>
        <span class="n">x_min</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="n">x_max</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

        <span class="n">y_min</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="n">y_max</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

        <span class="c1"># Generate predefined numbe of values for eaxh axis betwing correspondent min and max values.</span>
        <span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">predictions_count</span><span class="p">)</span>
        <span class="n">y_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">predictions_count</span><span class="p">)</span>

        <span class="c1"># Create empty vectors for X and Y axes predictions</span>
        <span class="c1"># We&#39;re going to find cartesian product of all possible X and Y values.</span>
        <span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">predictions_count</span> <span class="o">*</span> <span class="n">predictions_count</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">predictions_count</span> <span class="o">*</span> <span class="n">predictions_count</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Find cartesian product of all X and Y values.</span>
        <span class="n">x_y_index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">x_index</span><span class="p">,</span> <span class="n">x_value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x_axis</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">y_index</span><span class="p">,</span> <span class="n">y_value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_axis</span><span class="p">):</span>
                <span class="n">x_pred</span><span class="p">[</span><span class="n">x_y_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_value</span>
                <span class="n">y_pred</span><span class="p">[</span><span class="n">x_y_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_value</span>
                <span class="n">x_y_index</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Predict Z value for all X and Y pairs.</span>
        <span class="n">z_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>

        <span class="c1"># Plot training data with predictions.</span>

        <span class="c1"># Configure the plot with test dataset.</span>
        <span class="n">plot_predictions_trace</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
            <span class="n">y</span><span class="o">=</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
            <span class="n">z</span><span class="o">=</span><span class="n">z_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Prediction Plane&quot;</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;markers&quot;</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,},</span>
            <span class="n">opacity</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
            <span class="n">surfaceaxis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">plot_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">plot_training_trace</span><span class="p">,</span> <span class="n">plot_predictions_trace</span><span class="p">]</span>

    <span class="c1"># Configure the layout.</span>
    <span class="n">plot_layout</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Layout</span><span class="p">(</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Date Sets&quot;</span><span class="p">,</span>
        <span class="n">scene</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;xaxis&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">input_features</span><span class="p">[</span><span class="mi">0</span><span class="p">]},</span>
            <span class="s2">&quot;yaxis&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">input_features</span><span class="p">[</span><span class="mi">1</span><span class="p">]},</span>
            <span class="s2">&quot;zaxis&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">target_feature</span><span class="p">},</span>
        <span class="p">},</span>
        <span class="n">margin</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;l&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;t&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="p">)</span>

    <span class="n">plot_figure</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">plot_data</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">plot_layout</span><span class="p">)</span>

    <span class="c1"># Render 3D scatter plot.</span>
    <span class="n">plotly</span><span class="o">.</span><span class="n">offline</span><span class="o">.</span><span class="n">iplot</span><span class="p">(</span><span class="n">plot_figure</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot training data</span>
<span class="n">plot_multivariate</span><span class="p">(</span><span class="n">x_train_multi</span><span class="p">,</span> <span class="n">y_train_multi</span><span class="p">,</span> <span class="n">input_features_multi</span><span class="p">,</span> <span class="n">target_feature</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>                            <div id="ca2140c4-2fe8-4d54-8342-9780f887e2a5" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ca2140c4-2fe8-4d54-8342-9780f887e2a5")) {                    Plotly.newPlot(                        "ca2140c4-2fe8-4d54-8342-9780f887e2a5",                        [{"marker":{"line":{"color":"rgb(255, 255, 255)","width":1},"opacity":1,"size":10},"mode":"markers","name":"Actual","x":[0.305444717407227,1.43362653255463,1.12843120098114,1.15655755996704,1.54625928401947,0.0,0.989701807498932,1.36135590076447,0.596220076084137,1.15318381786346,0.244549930095673,1.55167484283447,0.728870630264282,1.484414935112,1.12209415435791,0.89465194940567,1.06931757926941,0.648457288742065,0.305808693170547,1.1982102394104,0.716249227523804,0.521021246910095,1.15360176563263,0.591683447360992,0.233442038297653,1.46378076076508,0.996192753314972,1.21755969524384,1.28948748111725,0.511135876178741,0.375846534967422,1.37538242340088,1.44357192516327,1.07498753070831,0.0226431842893362,1.40167844295502,1.48238301277161,0.479309022426605,1.74194359779358,1.41691517829895,0.381430715322495,0.368745893239975,1.02723586559296,0.72688353061676,0.786441087722778,0.368610262870789,0.560479462146759,0.991012394428253,0.964434325695038,0.524713635444641,1.28177809715271,1.23374843597412,0.878114581108093,1.48841226100922,1.3151752948761,0.091622568666935,0.636406779289246,1.39506661891937,0.601765096187592,1.00082039833069,0.730573117733002,0.0921023488044739,1.10180306434631,1.09186446666718,1.31458234786987,1.13077676296234,0.564305365085602,1.29121541976929,1.38439786434174,0.792221248149872,1.15687310695648,1.03522527217865,1.18529546260834,0.833756566047668,1.25278460979462,1.35268235206604,0.808964252471924,1.44163393974304,0.950612664222717,1.29178786277771,1.10271048545837,1.07937383651733,0.603048920631409,1.47920441627502,1.3412059545517,1.48792338371277,0.586682975292206,1.07062232494354,0.907975316047668,0.36842092871666,1.10970628261566,1.05469870567322,1.53570663928986,0.872001945972443,0.900596737861633,1.21768391132355,1.3469113111496,1.62634336948395,0.161925330758095,0.401477217674255,0.885416388511658,1.87076568603516,1.19827437400818,0.65951669216156,0.788547575473785,1.49438726902008,1.12112903594971,0.932537317276001,0.119041793048382,1.50394463539124,1.56497955322266,0.431085407733917,1.06457793712616,1.480633020401,1.69227766990662,1.63295245170593,1.12786877155304,1.53062355518341,1.32539355754852,1.10735321044922,0.909784495830536,1.1614590883255,0.925579309463501,0.339233845472336],"y":[0.38042613863945,0.361466586589813,0.153997123241425,0.295400261878967,0.505740523338318,0.270842045545578,0.282110154628754,0.518630743026733,0.454943388700485,0.412730008363724,0.348587512969971,0.490968644618988,0.240729048848152,0.601607382297516,0.505196332931519,0.122974775731564,0.208715528249741,0.0960980430245399,0.189196765422821,0.312328577041626,0.25471106171608,0.390661299228668,0.398155838251114,0.249463722109795,0.466914653778076,0.539770722389221,0.381498634815216,0.57939225435257,0.0957312509417534,0.390017777681351,0.336384207010269,0.405988603830338,0.617950856685638,0.288515985012054,0.602126955986023,0.257921665906906,0.626006722450256,0.377922266721725,0.59662789106369,0.505625545978546,0.443185955286026,0.581843852996826,0.394143968820572,0.23521526157856,0.658248662948608,0.0303698573261499,0.452763766050339,0.418421149253845,0.520303547382355,0.471566706895828,0.373783111572266,0.550026834011078,0.408158332109451,0.536746919155121,0.498465299606323,0.0599007532000542,0.461603492498398,0.256450712680817,0.633375823497772,0.4551981985569,0.348079860210419,0.235961347818375,0.465733230113983,0.233335807919502,0.234231784939766,0.41827192902565,0.430388748645782,0.402264982461929,0.408781230449677,0.469987004995346,0.249322608113289,0.450002878904343,0.494519203901291,0.558732926845551,0.376895278692245,0.490946173667908,0.435025870800018,0.508190035820007,0.309410035610199,0.520342111587524,0.288555532693863,0.55258983373642,0.447706192731857,0.611100912094116,0.572575807571411,0.562511384487152,0.478356659412384,0.477487415075302,0.547509372234344,0.318697690963745,0.580131649971008,0.479246735572815,0.573110342025757,0.531310617923737,0.198303267359734,0.457003742456436,0.471203625202179,0.60834527015686,0.36365869641304,0.106179520487785,0.501537680625916,0.604130983352661,0.300740599632263,0.0149958552792668,0.571055591106415,0.612924098968506,0.194989055395126,0.473507791757584,0.332881182432175,0.585384488105774,0.620070576667786,0.425962775945663,0.325905978679657,0.627162635326385,0.549840569496155,0.496337592601776,0.580200731754303,0.449750572443008,0.295817464590073,0.437453746795654,0.432452529668808,0.289231717586517,0.474307239055634,0.408842742443085],"z":[3.49499988555908,6.42199993133545,5.25,5.56899976730347,6.99300003051758,2.69300007820129,4.7350001335144,6.16800022125244,5.00400018692017,6.57800006866455,3.50699996948242,5.47200012207031,5.83799982070923,7.28399991989136,3.76600003242493,4.09600019454956,5.39499998092651,4.29199981689453,3.64400005340576,4.46500015258789,4.77500009536743,3.80800008773804,5.23400020599365,3.59299993515015,3.97000002861023,6.89099979400635,4.64400005340576,6.4539999961853,5.22700023651123,3.34899997711182,3.875,7.21299982070923,7.4689998626709,5.22499990463257,5.15100002288818,5.83799982070923,7.52199983596802,4.53499984741211,6.86299991607666,5.92000007629395,4.08099985122681,3.47099995613098,4.95499992370605,5.26900005340576,5.97100019454956,3.6029999256134,4.55299997329712,5.33599996566772,4.57399988174438,5.04099988937378,5.96299982070923,6.4520001411438,5.2350001335144,6.08699989318848,5.19500017166138,2.90499997138977,4.51399993896484,5.96400022506714,4.16800022125244,6.00799989700317,5.18100023269653,4.28000020980835,5.52500009536743,5.87200021743774,5.90199995040894,5.82200002670288,4.69500017166138,6.08400011062622,6.40299987792969,4.31500005722046,4.69199991226196,5.71500015258789,6.59899997711182,5.82299995422363,6.65199995040894,6.60900020599365,4.29099988937378,6.71400022506714,4.28599977493286,5.97300004959106,4.49700021743774,5.23000001907349,4.17999982833862,7.31599998474121,5.75799989700317,6.95100021362305,4.60799980163574,6.35699987411499,5.95599985122681,4.70900011062622,7.0789999961853,4.8289999961853,6.97700023651123,6.4539999961853,4.37599992752075,5.82499980926514,5.80999994277954,6.64799976348877,4.02799987792969,3.79399991035461,5.01100015640259,6.375,5.5,4.13899993896484,5.07399988174438,7.28399991989136,5.23699998855591,5.49300003051758,3.53299999237061,7.3769998550415,7.49399995803833,3.65700006484985,5.17500019073486,7.50400018692017,6.57200002670288,6.10500001907349,6.42399978637695,6.3439998626709,6.09800004959106,6.63500022888184,6.00299978256226,4.71400022506714,5.31099987030029,4.46000003814697],"type":"scatter3d"}],                        {"margin":{"b":0,"l":0,"r":0,"t":0},"scene":{"xaxis":{"title":{"text":"Economy..GDP.per.Capita."}},"yaxis":{"title":{"text":"Freedom"}},"zaxis":{"title":{"text":"Happiness.Score"}}},"template":{"data":{"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Date Sets"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('ca2140c4-2fe8-4d54-8342-9780f887e2a5');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Linear Regression model (based on Normal Equation)</span>
<span class="n">lr_model_multi</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Train and test the model on multivariate data</span>
<span class="n">train_model</span><span class="p">(</span><span class="n">lr_model_multi</span><span class="p">,</span> <span class="n">x_train_multi</span><span class="p">,</span> <span class="n">y_train_multi</span><span class="p">)</span>
<span class="n">test_model</span><span class="p">(</span><span class="n">lr_model_multi</span><span class="p">,</span> <span class="n">x_test_multi</span><span class="p">,</span> <span class="n">y_test_multi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [1.91149998 2.16361139], bias: 2.5932961237511147
Training error: 0.35373
Test error: 0.21432
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot data and model prediction</span>
<span class="n">plot_multivariate</span><span class="p">(</span><span class="n">x_train_multi</span><span class="p">,</span> <span class="n">y_train_multi</span><span class="p">,</span> <span class="n">input_features_multi</span><span class="p">,</span> <span class="n">target_feature</span><span class="p">,</span> <span class="n">lr_model_multi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>                            <div id="b8e51d29-4533-4e80-8594-bdc526ab1bae" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("b8e51d29-4533-4e80-8594-bdc526ab1bae")) {                    Plotly.newPlot(                        "b8e51d29-4533-4e80-8594-bdc526ab1bae",                        [{"marker":{"line":{"color":"rgb(255, 255, 255)","width":1},"opacity":1,"size":10},"mode":"markers","name":"Actual","x":[0.305444717407227,1.43362653255463,1.12843120098114,1.15655755996704,1.54625928401947,0.0,0.989701807498932,1.36135590076447,0.596220076084137,1.15318381786346,0.244549930095673,1.55167484283447,0.728870630264282,1.484414935112,1.12209415435791,0.89465194940567,1.06931757926941,0.648457288742065,0.305808693170547,1.1982102394104,0.716249227523804,0.521021246910095,1.15360176563263,0.591683447360992,0.233442038297653,1.46378076076508,0.996192753314972,1.21755969524384,1.28948748111725,0.511135876178741,0.375846534967422,1.37538242340088,1.44357192516327,1.07498753070831,0.0226431842893362,1.40167844295502,1.48238301277161,0.479309022426605,1.74194359779358,1.41691517829895,0.381430715322495,0.368745893239975,1.02723586559296,0.72688353061676,0.786441087722778,0.368610262870789,0.560479462146759,0.991012394428253,0.964434325695038,0.524713635444641,1.28177809715271,1.23374843597412,0.878114581108093,1.48841226100922,1.3151752948761,0.091622568666935,0.636406779289246,1.39506661891937,0.601765096187592,1.00082039833069,0.730573117733002,0.0921023488044739,1.10180306434631,1.09186446666718,1.31458234786987,1.13077676296234,0.564305365085602,1.29121541976929,1.38439786434174,0.792221248149872,1.15687310695648,1.03522527217865,1.18529546260834,0.833756566047668,1.25278460979462,1.35268235206604,0.808964252471924,1.44163393974304,0.950612664222717,1.29178786277771,1.10271048545837,1.07937383651733,0.603048920631409,1.47920441627502,1.3412059545517,1.48792338371277,0.586682975292206,1.07062232494354,0.907975316047668,0.36842092871666,1.10970628261566,1.05469870567322,1.53570663928986,0.872001945972443,0.900596737861633,1.21768391132355,1.3469113111496,1.62634336948395,0.161925330758095,0.401477217674255,0.885416388511658,1.87076568603516,1.19827437400818,0.65951669216156,0.788547575473785,1.49438726902008,1.12112903594971,0.932537317276001,0.119041793048382,1.50394463539124,1.56497955322266,0.431085407733917,1.06457793712616,1.480633020401,1.69227766990662,1.63295245170593,1.12786877155304,1.53062355518341,1.32539355754852,1.10735321044922,0.909784495830536,1.1614590883255,0.925579309463501,0.339233845472336],"y":[0.38042613863945,0.361466586589813,0.153997123241425,0.295400261878967,0.505740523338318,0.270842045545578,0.282110154628754,0.518630743026733,0.454943388700485,0.412730008363724,0.348587512969971,0.490968644618988,0.240729048848152,0.601607382297516,0.505196332931519,0.122974775731564,0.208715528249741,0.0960980430245399,0.189196765422821,0.312328577041626,0.25471106171608,0.390661299228668,0.398155838251114,0.249463722109795,0.466914653778076,0.539770722389221,0.381498634815216,0.57939225435257,0.0957312509417534,0.390017777681351,0.336384207010269,0.405988603830338,0.617950856685638,0.288515985012054,0.602126955986023,0.257921665906906,0.626006722450256,0.377922266721725,0.59662789106369,0.505625545978546,0.443185955286026,0.581843852996826,0.394143968820572,0.23521526157856,0.658248662948608,0.0303698573261499,0.452763766050339,0.418421149253845,0.520303547382355,0.471566706895828,0.373783111572266,0.550026834011078,0.408158332109451,0.536746919155121,0.498465299606323,0.0599007532000542,0.461603492498398,0.256450712680817,0.633375823497772,0.4551981985569,0.348079860210419,0.235961347818375,0.465733230113983,0.233335807919502,0.234231784939766,0.41827192902565,0.430388748645782,0.402264982461929,0.408781230449677,0.469987004995346,0.249322608113289,0.450002878904343,0.494519203901291,0.558732926845551,0.376895278692245,0.490946173667908,0.435025870800018,0.508190035820007,0.309410035610199,0.520342111587524,0.288555532693863,0.55258983373642,0.447706192731857,0.611100912094116,0.572575807571411,0.562511384487152,0.478356659412384,0.477487415075302,0.547509372234344,0.318697690963745,0.580131649971008,0.479246735572815,0.573110342025757,0.531310617923737,0.198303267359734,0.457003742456436,0.471203625202179,0.60834527015686,0.36365869641304,0.106179520487785,0.501537680625916,0.604130983352661,0.300740599632263,0.0149958552792668,0.571055591106415,0.612924098968506,0.194989055395126,0.473507791757584,0.332881182432175,0.585384488105774,0.620070576667786,0.425962775945663,0.325905978679657,0.627162635326385,0.549840569496155,0.496337592601776,0.580200731754303,0.449750572443008,0.295817464590073,0.437453746795654,0.432452529668808,0.289231717586517,0.474307239055634,0.408842742443085],"z":[3.49499988555908,6.42199993133545,5.25,5.56899976730347,6.99300003051758,2.69300007820129,4.7350001335144,6.16800022125244,5.00400018692017,6.57800006866455,3.50699996948242,5.47200012207031,5.83799982070923,7.28399991989136,3.76600003242493,4.09600019454956,5.39499998092651,4.29199981689453,3.64400005340576,4.46500015258789,4.77500009536743,3.80800008773804,5.23400020599365,3.59299993515015,3.97000002861023,6.89099979400635,4.64400005340576,6.4539999961853,5.22700023651123,3.34899997711182,3.875,7.21299982070923,7.4689998626709,5.22499990463257,5.15100002288818,5.83799982070923,7.52199983596802,4.53499984741211,6.86299991607666,5.92000007629395,4.08099985122681,3.47099995613098,4.95499992370605,5.26900005340576,5.97100019454956,3.6029999256134,4.55299997329712,5.33599996566772,4.57399988174438,5.04099988937378,5.96299982070923,6.4520001411438,5.2350001335144,6.08699989318848,5.19500017166138,2.90499997138977,4.51399993896484,5.96400022506714,4.16800022125244,6.00799989700317,5.18100023269653,4.28000020980835,5.52500009536743,5.87200021743774,5.90199995040894,5.82200002670288,4.69500017166138,6.08400011062622,6.40299987792969,4.31500005722046,4.69199991226196,5.71500015258789,6.59899997711182,5.82299995422363,6.65199995040894,6.60900020599365,4.29099988937378,6.71400022506714,4.28599977493286,5.97300004959106,4.49700021743774,5.23000001907349,4.17999982833862,7.31599998474121,5.75799989700317,6.95100021362305,4.60799980163574,6.35699987411499,5.95599985122681,4.70900011062622,7.0789999961853,4.8289999961853,6.97700023651123,6.4539999961853,4.37599992752075,5.82499980926514,5.80999994277954,6.64799976348877,4.02799987792969,3.79399991035461,5.01100015640259,6.375,5.5,4.13899993896484,5.07399988174438,7.28399991989136,5.23699998855591,5.49300003051758,3.53299999237061,7.3769998550415,7.49399995803833,3.65700006484985,5.17500019073486,7.50400018692017,6.57200002670288,6.10500001907349,6.42399978637695,6.3439998626709,6.09800004959106,6.63500022888184,6.00299978256226,4.71400022506714,5.31099987030029,4.46000003814697],"type":"scatter3d"},{"marker":{"size":1},"mode":"markers","name":"Prediction Plane","opacity":0.8,"surfaceaxis":2,"x":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20786285400390667,0.20786285400390667,0.20786285400390667,0.20786285400390667,0.20786285400390667,0.20786285400390667,0.20786285400390667,0.20786285400390667,0.20786285400390667,0.20786285400390667,0.41572570800781333,0.41572570800781333,0.41572570800781333,0.41572570800781333,0.41572570800781333,0.41572570800781333,0.41572570800781333,0.41572570800781333,0.41572570800781333,0.41572570800781333,0.62358856201172,0.62358856201172,0.62358856201172,0.62358856201172,0.62358856201172,0.62358856201172,0.62358856201172,0.62358856201172,0.62358856201172,0.62358856201172,0.8314514160156267,0.8314514160156267,0.8314514160156267,0.8314514160156267,0.8314514160156267,0.8314514160156267,0.8314514160156267,0.8314514160156267,0.8314514160156267,0.8314514160156267,1.0393142700195332,1.0393142700195332,1.0393142700195332,1.0393142700195332,1.0393142700195332,1.0393142700195332,1.0393142700195332,1.0393142700195332,1.0393142700195332,1.0393142700195332,1.24717712402344,1.24717712402344,1.24717712402344,1.24717712402344,1.24717712402344,1.24717712402344,1.24717712402344,1.24717712402344,1.24717712402344,1.24717712402344,1.4550399780273466,1.4550399780273466,1.4550399780273466,1.4550399780273466,1.4550399780273466,1.4550399780273466,1.4550399780273466,1.4550399780273466,1.4550399780273466,1.4550399780273466,1.6629028320312533,1.6629028320312533,1.6629028320312533,1.6629028320312533,1.6629028320312533,1.6629028320312533,1.6629028320312533,1.6629028320312533,1.6629028320312533,1.6629028320312533,1.87076568603516,1.87076568603516,1.87076568603516,1.87076568603516,1.87076568603516,1.87076568603516,1.87076568603516,1.87076568603516,1.87076568603516,1.87076568603516],"y":[0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608,0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608,0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608,0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608,0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608,0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608,0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608,0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608,0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608,0.0149958552792668,0.08646838946474913,0.1579409236502315,0.22941345783571385,0.30088599202119615,0.3723585262066785,0.44383106039216086,0.5153035945776432,0.5867761287631256,0.658248662948608],"z":[2.6257413270521166,2.7803801161641943,2.9350189052762725,3.0896576943883503,3.2442964835004284,3.398935272612506,3.553574061724584,3.708212850836662,3.86285163994874,4.0174904290608175,3.023071168130305,3.177709957242383,3.332348746354461,3.4869875354665387,3.641626324578617,3.796265113690694,3.9509039028027724,4.1055426919148506,4.260181481026928,4.414820270139007,3.4204010092084935,3.5750397983205713,3.729678587432649,3.884317376544727,4.038956165656805,4.193594954768883,4.348233743880961,4.502872532993039,4.657511322105117,4.8121501112171945,3.817730850286682,3.9723696393987598,4.1270084285108375,4.281647217622916,4.436286006734994,4.590924795847071,4.745563584959149,4.9002023740712275,5.054841163183305,5.209479952295384,4.2150606913648705,4.369699480476948,4.524338269589026,4.678977058701104,4.833615847813181,4.98825463692526,5.142893426037338,5.297532215149415,5.452171004261494,5.606809793373571,4.612390532443058,4.767029321555136,4.921668110667214,5.076306899779292,5.230945688891371,5.385584478003448,5.540223267115525,5.694862056227604,5.849500845339682,6.00413963445176,5.009720373521247,5.164359162633325,5.318997951745403,5.473636740857481,5.628275529969558,5.782914319081637,5.937553108193715,6.092191897305792,6.24683068641787,6.401469475529948,5.407050214599435,5.561689003711513,5.716327792823591,5.870966581935669,6.025605371047748,6.180244160159825,6.334882949271902,6.489521738383981,6.644160527496059,6.798799316608137,5.804380055677624,5.959018844789702,6.11365763390178,6.268296423013858,6.422935212125935,6.5775740012380135,6.732212790350092,6.886851579462169,7.041490368574247,7.196129157686325,6.201709896755812,6.35634868586789,6.510987474979968,6.6656262640920465,6.820265053204124,6.974903842316202,7.12954263142828,7.284181420540357,7.438820209652436,7.593458998764514],"type":"scatter3d"}],                        {"margin":{"b":0,"l":0,"r":0,"t":0},"scene":{"xaxis":{"title":{"text":"Economy..GDP.per.Capita."}},"yaxis":{"title":{"text":"Freedom"}},"zaxis":{"title":{"text":"Happiness.Score"}}},"template":{"data":{"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Date Sets"}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('b8e51d29-4533-4e80-8594-bdc526ab1bae');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div></div></div>
</div>
</div>
<div class="section" id="pros-cons-of-analytical-approach">
<h3>Pros/cons of analytical approach<a class="headerlink" href="#pros-cons-of-analytical-approach" title="Permalink to this headline">Â¶</a></h3>
<p>Pros:</p>
<ul class="simple">
<li><p>Computed in one step.</p></li>
<li><p>Exact solution.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>Computation of <span class="math notranslate nohighlight">\((\pmb{X}^T\pmb{X})^{-1}\)</span> is slow when the number of features is large (<span class="math notranslate nohighlight">\(n &gt; 10^4\)</span>).</p></li>
<li><p>Doesnâ€™t work if <span class="math notranslate nohighlight">\(\pmb{X}^T\pmb{X}\)</span> is not inversible.</p></li>
</ul>
</div>
</div>
<div class="section" id="iterative-approach-gradient-descent">
<h2>Iterative approach: gradient descent<a class="headerlink" href="#iterative-approach-gradient-descent" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="method-description">
<h3>Method description<a class="headerlink" href="#method-description" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>Same objective: find the <span class="math notranslate nohighlight">\(\pmb{\theta}^{*}\)</span> vector that minimizes the loss.</p></li>
<li><p>General idea:</p>
<ul>
<li><p>Start with random values for <span class="math notranslate nohighlight">\(\pmb{\theta}\)</span>.</p></li>
<li><p>Update <span class="math notranslate nohighlight">\(\pmb{\theta}\)</span> in small steps towards loss minimization.</p></li>
</ul>
</li>
<li><p>To know in which direction update <span class="math notranslate nohighlight">\(\pmb{\theta}\)</span>, we compute the <strong>gradient</strong> of the loss function w.r.t. <span class="math notranslate nohighlight">\(\pmb{\theta}\)</span> and go into the opposite direction.</p></li>
</ul>
</div>
<div class="section" id="computation-of-gradients">
<h3>Computation of gradients<a class="headerlink" href="#computation-of-gradients" title="Permalink to this headline">Â¶</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla_{\theta}\mathcal{L}(\pmb{\theta}) = \begin{pmatrix}
       \ \frac{\partial}{\partial \theta_0} \mathcal{L}(\theta) \\
       \ \frac{\partial}{\partial \theta_1} \mathcal{L}(\theta) \\
       \ \vdots \\
       \ \frac{\partial}{\partial \theta_n} \mathcal{L}(\theta)
     \end{pmatrix} =
\frac{2}{m}\pmb{X}^T\left(\pmb{X}\pmb{\theta} - \pmb{y}\right)\end{split}\]</div>
<p>(See math proof above)</p>
</div>
<div class="section" id="parameters-update">
<h3>Parameters update<a class="headerlink" href="#parameters-update" title="Permalink to this headline">Â¶</a></h3>
<p>The <strong><em>learning rate</em></strong> <span class="math notranslate nohighlight">\(\eta\)</span> governs the amplitude of updates.</p>
<div class="math notranslate nohighlight">
\[\pmb{\theta}_{next} = \pmb{\theta} - \eta\nabla_{\theta}\mathcal{L}(\pmb{\theta})\]</div>
<p><a class="reference external" href="https://www.jeremyjordan.me/nn-learning-rate/"><img alt="Importance of learning rate" src="images/learning_rate.png" /></a></p>
</div>
<div class="section" id="interactive-example">
<h3>Interactive example<a class="headerlink" href="#interactive-example" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference external" href="https://alykhantejani.github.io/a-brief-introduction-to-gradient-descent/"><img alt="Gradient descent line graph" src="images/gradient_descent_line_graph.gif" /></a></p>
</div>
<div class="section" id="gradient-descent-types">
<h3>Gradient descent types<a class="headerlink" href="#gradient-descent-types" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p><em>Batch</em>: use the whole dataset to compute gradients.</p>
<ul>
<li><p>Safe but potentially slow.</p></li>
</ul>
</li>
<li><p><em>Stochastic</em>: use only one sample.</p>
<ul>
<li><p>Fast but potentially erratic.</p></li>
</ul>
</li>
<li><p><em>Mini-Batch</em>: use a small set of samples (10-1000).</p>
<ul>
<li><p>Good compromise.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="example-applying-stochastic-gradient-descent-to-predict-country-happiness">
<h3>Example: applying stochastic gradient descent to predict country happiness<a class="headerlink" href="#example-applying-stochastic-gradient-descent-to-predict-country-happiness" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Linear Regression model (based on Stochastic Gradient Descent)</span>
<span class="n">sgd_model_uni</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">()</span>

<span class="c1"># Train and test the model on univariate data</span>
<span class="n">train_model</span><span class="p">(</span><span class="n">sgd_model_uni</span><span class="p">,</span> <span class="n">x_train_uni</span><span class="p">,</span> <span class="n">y_train_uni</span><span class="p">)</span>
<span class="n">test_model</span><span class="p">(</span><span class="n">sgd_model_uni</span><span class="p">,</span> <span class="n">x_test_uni</span><span class="p">,</span> <span class="n">y_test_uni</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [2.53591725], bias: [2.5963995]
Training error: 0.50506
Test error: 0.58853
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot data and models predictions</span>
<span class="n">plot_univariate</span><span class="p">(</span>
    <span class="n">x_train_uni</span><span class="p">,</span>
    <span class="n">y_train_uni</span><span class="p">,</span>
    <span class="n">input_features_uni</span><span class="p">,</span>
    <span class="n">target_feature</span><span class="p">,</span>
    <span class="n">model_list</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;LR&quot;</span><span class="p">:</span> <span class="n">lr_model</span><span class="p">,</span> <span class="s2">&quot;SGD&quot;</span><span class="p">:</span> <span class="n">sgd_model_uni</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_linear_regression_72_0.png" src="_images/2_linear_regression_72_0.png" />
</div>
</div>
</div>
<div class="section" id="pros-cons-of-iterative-approach">
<h3>Pros/cons of iterative approach<a class="headerlink" href="#pros-cons-of-iterative-approach" title="Permalink to this headline">Â¶</a></h3>
<p>Pros:</p>
<ul class="simple">
<li><p>Works well with a large number of features.</p></li>
<li><p>MSE loss function convex =&gt; guarantee of a global minimum.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>Convergence depends on learning rate and GD type.</p></li>
<li><p>Dependant on feature scaling.</p></li>
</ul>
</div>
</div>
<div class="section" id="polynomial-regression">
<h2>Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="general-idea">
<h3>General idea<a class="headerlink" href="#general-idea" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>How can a linear model fit non-linear data?</p></li>
<li><p>Solution: add powers of each feature as new features.</p></li>
<li><p>The hypothesis function is still linear.</p></li>
<li><p>High-degree polynomial regression can be subject to severe overfitting.</p></li>
</ul>
</div>
<div class="section" id="example-fitting-a-quadratic-curve-with-polynomial-regression">
<h3>Example: fitting a quadratic curve with polynomial regression<a class="headerlink" href="#example-fitting-a-quadratic-curve-with-polynomial-regression" title="Permalink to this headline">Â¶</a></h3>
<p>(Heavily inspired by Chapter 4 of <a class="reference external" href="https://github.com/ageron/handson-ml2">Hands-On Machine Learning</a> by AurÃ©lien GÃ©ron)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate quadratic data with noise</span>
<span class="c1"># (ripped from https://github.com/ageron/handson-ml2)</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">200</span>
<span class="c1"># Generate m data samples between -3 and 3</span>
<span class="n">x_quad</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span>

<span class="c1"># y = 0,5x^2 + x + 2 + noise</span>
<span class="n">y_quad</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_quad</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x_quad</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot generated data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_quad</span><span class="p">,</span> <span class="n">y_quad</span><span class="p">,</span> <span class="s2">&quot;b.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Quadratic data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_linear_regression_78_0.png" src="_images/2_linear_regression_78_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial feature for first sample: </span><span class="si">{</span><span class="n">x_quad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Add polynomial features to the dataset</span>
<span class="n">poly_degree</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">poly_features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">poly_degree</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">x_quad_poly</span> <span class="o">=</span> <span class="n">poly_features</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_quad</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;New features for first sample: </span><span class="si">{</span><span class="n">x_quad_poly</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial feature for first sample: [1.80326168]
New features for first sample: [1.80326168 3.25175268]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit a linear regression model to the extended data</span>
<span class="n">lr_model_poly</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr_model_poly</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_quad_poly</span><span class="p">,</span> <span class="n">y_quad</span><span class="p">)</span>

<span class="c1"># Should be close to [1, 0,5] and 2</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model weights: </span><span class="si">{</span><span class="n">lr_model_poly</span><span class="o">.</span><span class="n">coef_</span><span class="si">}</span><span class="s2">, bias: </span><span class="si">{</span><span class="n">lr_model_poly</span><span class="o">.</span><span class="n">intercept_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [[0.99406219 0.49278801]], bias: [1.97885369]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot data and model prediction</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_quad</span><span class="p">,</span> <span class="n">y_quad</span><span class="p">,</span> <span class="s2">&quot;b.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data&quot;</span><span class="p">)</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_pred_poly</span> <span class="o">=</span> <span class="n">poly_features</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr_model_poly</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_pred_poly</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Prediction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Quadratic data w/ prediction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_linear_regression_81_0.png" src="_images/2_linear_regression_81_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="regularization">
<h2>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="id2">
<h3>General idea<a class="headerlink" href="#id2" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>Solution against overfitting: constraining model parameters to take small values.</p></li>
<li><p><em>Ridge regression</em> (using l2 norm):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\pmb{\theta}) = \mathrm{MSE}(\pmb{\theta}) + \frac{\lambda}{2}\sum_{i=1}^n \theta_i^2\]</div>
<ul class="simple">
<li><p><em>Lasso regression</em> (using l1 norm):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\pmb{\theta}) = \mathrm{MSE}(\pmb{\theta}) + \lambda\sum_{i=1}^n |\theta_i|\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> is called the <strong>regularization rate</strong>.</p></li>
</ul>
</div>
<div class="section" id="example-observing-the-effects-of-regularization-rate">
<h3>Example: observing the effects of regularization rate<a class="headerlink" href="#example-observing-the-effects-of-regularization-rate" title="Permalink to this headline">Â¶</a></h3>
<p>(Heavily inspired by Chapter 4 of <a class="reference external" href="https://github.com/ageron/handson-ml2">Hands-On Machine Learning</a> by AurÃ©lien GÃ©ron)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">x_reg</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_reg</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_reg</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1.5</span>
<span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_model</span><span class="p">(</span><span class="n">model_class</span><span class="p">,</span> <span class="n">polynomial</span><span class="p">,</span> <span class="n">lambdas</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kargs</span><span class="p">):</span>
    <span class="c1"># Plot data and predictions for several regularization rates</span>
    <span class="k">for</span> <span class="n">reg_rate</span><span class="p">,</span> <span class="n">style</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span> <span class="s2">&quot;r:&quot;</span><span class="p">)):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_class</span><span class="p">(</span><span class="n">reg_rate</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kargs</span><span class="p">)</span> <span class="k">if</span> <span class="n">reg_rate</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">LinearRegression</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">polynomial</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
                    <span class="p">(</span><span class="s2">&quot;poly_features&quot;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
                    <span class="p">(</span><span class="s2">&quot;std_scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                    <span class="p">(</span><span class="s2">&quot;regul_reg&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">),</span>
                <span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_reg</span><span class="p">,</span> <span class="n">y_reg</span><span class="p">)</span>
        <span class="n">y_new_regul</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
        <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">reg_rate</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">y_new_regul</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\lambda = </span><span class="si">{}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg_rate</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_reg</span><span class="p">,</span> <span class="n">y_reg</span><span class="p">,</span> <span class="s2">&quot;b.&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot data and predictions with varying regularization rates</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_model</span><span class="p">(</span><span class="n">Ridge</span><span class="p">,</span> <span class="n">polynomial</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lambdas</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear regression with Ridge regularization&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plot_model</span><span class="p">(</span><span class="n">Ridge</span><span class="p">,</span> <span class="n">polynomial</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lambdas</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="o">**-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Polynomial regression with Ridge regularization&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_linear_regression_87_0.png" src="_images/2_linear_regression_87_0.png" />
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="1_Introduction_to_Machine_Learning.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">1. Introduction to Machine Learning</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3_Logistic_regression.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3. Logistic Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By <a href="https://fangli-ying.github.io/">Dr. Fangli Ying</a><br/>
    
        &copy; Copyright 2023.<br/>
      <div class="extra_footer">
        Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>